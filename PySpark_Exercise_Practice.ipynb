{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03f7f1b",
   "metadata": {},
   "source": [
    "# PySpark Practice Exercises\n",
    "\n",
    "**Author:** Benjamin Gao  \n",
    "**Date:** October 27, 2025  \n",
    "**Purpose:** Hands-on practice with PySpark DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Exercise Overview\n",
    "\n",
    "This notebook contains 6 comprehensive exercises:\n",
    "\n",
    "1. **Create & Read CSV** - Manually create CSV with customer data\n",
    "2. **Filter Data** - Find customers who bought more than 5 items\n",
    "3. **Add Columns** - Calculate TotalCost (Quantity Ã— Price)\n",
    "4. **Handle Missing Values** - Fill with averages and constants\n",
    "5. **Aggregations** - Group by Product and calculate statistics\n",
    "6. **Joins** - Join with Product details using inner, left, right joins\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86292461",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup: Environment Configuration\n",
    "\n",
    "Run this cell first to configure Java and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c777ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: JAVA_HOME + Import Libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set JAVA_HOME (adjust if needed)\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(f\"âœ… JAVA_HOME: {java_home}\")\n",
    "print(f\"âœ… Python: {sys.executable}\")\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"PySparkExercise\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.ui.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… Spark {spark.version} is running!\")\n",
    "print(f\"âœ… Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8117a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Create CSV File and Read into DataFrame\n",
    "\n",
    "**Task:** \n",
    "1. Manually create a CSV file with columns: `CustomerID`, `Product`, `Quantity`, `Price`\n",
    "2. Read the CSV file into a PySpark DataFrame\n",
    "\n",
    "**Expected Output:** DataFrame with customer purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982eaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create CSV data\n",
    "csv_data = \"\"\"CustomerID,Product,Quantity,Price\n",
    "C001,Laptop,2,1200.00\n",
    "C002,Mouse,10,25.50\n",
    "C003,Keyboard,5,80.00\n",
    "C004,Monitor,3,300.00\n",
    "C001,Mouse,8,25.50\n",
    "C002,Laptop,1,1200.00\n",
    "C005,Keyboard,,80.00\n",
    "C003,Monitor,4,\n",
    "C006,Laptop,6,1200.00\n",
    "C007,Mouse,12,25.50\n",
    "C004,Keyboard,3,80.00\n",
    "C008,Monitor,,300.00\n",
    "C005,Mouse,15,25.50\n",
    "C009,Laptop,1,1200.00\n",
    "C010,Keyboard,7,80.00\"\"\"\n",
    "\n",
    "# Step 2: Write to CSV file\n",
    "with open(\"customer_purchases.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"âœ… CSV file 'customer_purchases.csv' created!\")\n",
    "\n",
    "# Step 3: Read CSV into DataFrame\n",
    "df = spark.read.csv(\"customer_purchases.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Customer Purchases DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\nðŸ“‹ Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a4251",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Filter Customers Who Bought More Than 5 Items\n",
    "\n",
    "**Task:** Filter the DataFrame to show only customers who purchased more than 5 items (Quantity > 5)\n",
    "\n",
    "**Expected Output:** DataFrame with high-quantity purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: Quantity > 5\n",
    "df_filtered = df.filter(df.Quantity > 5)\n",
    "\n",
    "print(\"ðŸ” Customers who bought MORE than 5 items:\")\n",
    "df_filtered.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Total records: {df_filtered.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b67a33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Add TotalCost Column\n",
    "\n",
    "**Task:** Add a new column `TotalCost` calculated as `Quantity Ã— Price`\n",
    "\n",
    "**Expected Output:** DataFrame with TotalCost column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TotalCost column\n",
    "df_with_cost = df.withColumn(\"TotalCost\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "print(\"ðŸ’° DataFrame with TotalCost:\")\n",
    "df_with_cost.show()\n",
    "\n",
    "print(\"\\nðŸ“‹ Updated Schema:\")\n",
    "df_with_cost.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186c39a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Handle Missing Values\n",
    "\n",
    "**Task:** \n",
    "1. Fill missing `Quantity` with the **average** Quantity\n",
    "2. Fill missing `Price` with a **constant** value (e.g., 50.0)\n",
    "\n",
    "**Expected Output:** DataFrame with no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before\n",
    "print(\"âŒ Missing values BEFORE handling:\")\n",
    "df_with_cost.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df_with_cost.columns]\n",
    ").show()\n",
    "\n",
    "# Calculate average Quantity (for non-null values)\n",
    "avg_quantity = df_with_cost.select(avg(\"Quantity\")).first()[0]\n",
    "print(f\"\\nðŸ“Š Average Quantity: {avg_quantity:.2f}\")\n",
    "\n",
    "# Fill missing values\n",
    "df_filled = df_with_cost.fillna({\n",
    "    \"Quantity\": avg_quantity,  # Fill with average\n",
    "    \"Price\": 50.0               # Fill with constant\n",
    "})\n",
    "\n",
    "# Recalculate TotalCost after filling\n",
    "df_filled = df_filled.withColumn(\"TotalCost\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "print(\"\\nâœ… DataFrame AFTER filling missing values:\")\n",
    "df_filled.show()\n",
    "\n",
    "# Verify no missing values\n",
    "print(\"\\nâœ… Missing values AFTER handling:\")\n",
    "df_filled.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df_filled.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2f305",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Group by Product and Calculate Aggregations\n",
    "\n",
    "**Task:** Group by `Product` and calculate:\n",
    "- Average TotalCost\n",
    "- Sum of TotalCost\n",
    "- Min TotalCost\n",
    "- Max TotalCost\n",
    "- Count of records\n",
    "\n",
    "**Expected Output:** Aggregated statistics per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Product and aggregate\n",
    "df_aggregated = df_filled.groupBy(\"Product\").agg(\n",
    "    avg(\"TotalCost\").alias(\"Avg_TotalCost\"),\n",
    "    sum(\"TotalCost\").alias(\"Sum_TotalCost\"),\n",
    "    min(\"TotalCost\").alias(\"Min_TotalCost\"),\n",
    "    max(\"TotalCost\").alias(\"Max_TotalCost\"),\n",
    "    count(\"*\").alias(\"Record_Count\")\n",
    ").orderBy(\"Product\")\n",
    "\n",
    "print(\"ðŸ“Š Product Statistics:\")\n",
    "df_aggregated.show()\n",
    "\n",
    "# Optional: Show with formatted numbers\n",
    "print(\"\\nðŸ’° Formatted Statistics:\")\n",
    "df_aggregated.select(\n",
    "    col(\"Product\"),\n",
    "    format_number(\"Avg_TotalCost\", 2).alias(\"Avg ($)\"),\n",
    "    format_number(\"Sum_TotalCost\", 2).alias(\"Total ($)\"),\n",
    "    format_number(\"Min_TotalCost\", 2).alias(\"Min ($)\"),\n",
    "    format_number(\"Max_TotalCost\", 2).alias(\"Max ($)\"),\n",
    "    col(\"Record_Count\").alias(\"Count\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae4eeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: Join with Product Details\n",
    "\n",
    "**Task:** \n",
    "1. Create another CSV file with Product details (ProductID, Category)\n",
    "2. Perform **Inner**, **Left**, and **Right** joins\n",
    "\n",
    "**Expected Output:** Three different join results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Product Details CSV\n",
    "product_csv = \"\"\"ProductID,Category\n",
    "Laptop,Electronics\n",
    "Mouse,Electronics\n",
    "Keyboard,Electronics\n",
    "Monitor,Electronics\n",
    "Webcam,Electronics\"\"\"\n",
    "\n",
    "with open(\"product_details.csv\", \"w\") as f:\n",
    "    f.write(product_csv)\n",
    "\n",
    "print(\"âœ… CSV file 'product_details.csv' created!\")\n",
    "\n",
    "# Step 2: Read Product Details into DataFrame\n",
    "df_products = spark.read.csv(\"product_details.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"\\nðŸ“¦ Product Details:\")\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde24b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column for clarity (Product â†’ ProductID in df_filled)\n",
    "# Note: We'll join on Product name\n",
    "\n",
    "# INNER JOIN: Only matching products\n",
    "print(\"ðŸ”— INNER JOIN (only products in BOTH tables):\")\n",
    "df_inner = df_filled.join(\n",
    "    df_products, \n",
    "    df_filled.Product == df_products.ProductID, \n",
    "    how=\"inner\"\n",
    ").drop(\"ProductID\")  # Remove duplicate column\n",
    "\n",
    "df_inner.show()\n",
    "\n",
    "print(f\"Records: {df_inner.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN: All customer purchases + matching product details\n",
    "print(\"ðŸ”— LEFT JOIN (all purchases, with product details if available):\")\n",
    "df_left = df_filled.join(\n",
    "    df_products, \n",
    "    df_filled.Product == df_products.ProductID, \n",
    "    how=\"left\"\n",
    ").drop(\"ProductID\")\n",
    "\n",
    "df_left.show()\n",
    "\n",
    "print(f\"Records: {df_left.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc111d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT JOIN: All products + matching customer purchases\n",
    "print(\"ðŸ”— RIGHT JOIN (all products, with purchases if available):\")\n",
    "df_right = df_filled.join(\n",
    "    df_products, \n",
    "    df_filled.Product == df_products.ProductID, \n",
    "    how=\"right\"\n",
    ").drop(\"ProductID\")\n",
    "\n",
    "df_right.show()\n",
    "\n",
    "print(f\"Records: {df_right.count()}\")\n",
    "\n",
    "# Notice: \"Webcam\" appears with NULL values (no customer bought it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd42598",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Summary\n",
    "\n",
    "### What You've Practiced:\n",
    "\n",
    "âœ… **Exercise 1:** Created CSV and read into DataFrame  \n",
    "âœ… **Exercise 2:** Filtered data with conditions  \n",
    "âœ… **Exercise 3:** Added computed columns  \n",
    "âœ… **Exercise 4:** Handled missing values (avg & constant)  \n",
    "âœ… **Exercise 5:** Performed grouped aggregations  \n",
    "âœ… **Exercise 6:** Executed multiple join types  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”‘ Key Concepts:\n",
    "\n",
    "| Concept | Function Used |\n",
    "|---------|---------------|\n",
    "| **Read CSV** | `spark.read.csv()` |\n",
    "| **Filter** | `.filter()` or `.where()` |\n",
    "| **Add Column** | `.withColumn()` |\n",
    "| **Fill Missing** | `.fillna()` |\n",
    "| **Aggregation** | `.groupBy().agg()` |\n",
    "| **Joins** | `.join(how=\"inner/left/right\")` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Join Types Recap:\n",
    "\n",
    "- **Inner Join:** Only rows with matches in BOTH tables\n",
    "- **Left Join:** ALL rows from left table + matches from right\n",
    "- **Right Join:** ALL rows from right table + matches from left\n",
    "- **Full Outer Join:** ALL rows from both tables (not shown)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Challenge Tasks (Optional):\n",
    "\n",
    "1. **Calculate total revenue per customer**\n",
    "   - Group by CustomerID\n",
    "   - Sum TotalCost\n",
    "   - Find top 3 customers by revenue\n",
    "\n",
    "2. **Find the most popular product**\n",
    "   - Group by Product\n",
    "   - Sum Quantity\n",
    "   - Order by total quantity descending\n",
    "\n",
    "3. **Add discount column**\n",
    "   - 10% discount if Quantity > 10\n",
    "   - 5% discount if Quantity > 5\n",
    "   - Calculate final price after discount\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Practicing! ðŸŽ‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165991fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (optional): Stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"âœ… Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
