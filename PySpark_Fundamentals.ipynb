{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157592d8",
   "metadata": {},
   "source": [
    "# PySpark Fundamentals - Jupyter Notebook\n",
    "\n",
    "This notebook explains PySpark fundamentals for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006784e",
   "metadata": {},
   "source": [
    "## 0. Getting Started\n",
    "\n",
    "### 0.1 Acknowledgments\n",
    "\n",
    "**Original Source:** Professor Dr. Ali Safari  \n",
    "**Modified by:** Benjamin Gao (Enhanced structure and explanations for better learning experience)  \n",
    "**Email:** g103200@gmail.com \n",
    "\n",
    "---\n",
    "\n",
    "**‚òï Support This Work:**\n",
    "\n",
    "If you find this notebook helpful, consider buying me a coffee! Your support helps create more free educational content. üôè\n",
    "\n",
    "**üí≥ Fiat Currency:**\n",
    "- **Alipay/ÊîØ‰ªòÂÆù:** `g103200@gmail.com`\n",
    "- **Wise:** `g103200@gmail.com`\n",
    "- **PayPal:** [paypal.me/gbenjamin3](https://paypal.me/gbenjamin3)\n",
    "\n",
    "**‚Çø Cryptocurrency:**\n",
    "\n",
    "<details>\n",
    "<summary>Click to view crypto addresses</summary>\n",
    "\n",
    "- **ETH (ERC20):** `0x05bd3070993c1ef72b1ca3a06999cbcc3f61ad8b`\n",
    "- **USDT (ERC20):** `0x0a4649d6cbabf9bcf0419ac829f22a273136af51`\n",
    "- **SOL (Solana):** `3bsEtgBPeNwMrHLzQBrxiQ7wX1nr3dSRrzVAHoa1nudQ`\n",
    "- **BTC (Bitcoin):** `bc1ql0pafavp4l0l7j9m6dhgqajces3a80zqdj2kp8nua3aw4hqsm6vsnucv2m`\n",
    "\n",
    "</details>\n",
    "\n",
    "*Every contribution, no matter how small, is greatly appreciated!* ‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 How to Use This Notebook\n",
    "\n",
    "#### üìñ Folding Feature\n",
    "- Click **‚ñº** to collapse sections\n",
    "- Click **‚ñ∂** to expand content\n",
    "\n",
    "#### üéØ Learning Path\n",
    "1. Getting Started ‚Üí Setup environment\n",
    "2. Introduction ‚Üí PySpark basics\n",
    "3. Creating DataFrames ‚Üí Three creation methods\n",
    "4. Basic Operations ‚Üí Common operations\n",
    "5. Class Activity ‚Üí Hands-on practice\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 System Information\n",
    "\n",
    "**Local Environment (This Notebook):**\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|---------|\n",
    "| System | M4Pro MacBook Pro macOS 26 |\n",
    "| Python | 3.14.0 |\n",
    "| PySpark | 4.0.1 |\n",
    "| Java | OpenJDK 17 |\n",
    "| Environment | `~/.venvs/pyspark-latest` |\n",
    "\n",
    "**üí° Alternative: Google Colab**\n",
    "\n",
    "If you find local environment setup too complicated, consider using **Google Colab**:\n",
    "- ‚úÖ **Free** - No cost to use\n",
    "- ‚úÖ **Easy** - No installation needed, runs in browser\n",
    "- ‚úÖ **Powerful** - Free GPU/TPU access\n",
    "- ‚úÖ **Pre-configured** - PySpark ready to use with minimal setup\n",
    "\n",
    "**To use Colab:** Visit [colab.research.google.com](https://colab.research.google.com)\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 Kernel Selection\n",
    "\n",
    "**Steps:**\n",
    "1. Click \"Select Kernel\" (top-right corner)\n",
    "2. Choose **\"Python (PySpark latest)\"**\n",
    "3. Run the setup cell below to configure JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1821918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì JAVA_HOME set to: /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "‚úì Ready to run PySpark!\n",
      "‚úì Current Python: /opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/os.py\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è IMPORTANT: Set JAVA_HOME for PySpark (Run this FIRST!)\n",
    "import os\n",
    "\n",
    "# Set JAVA_HOME to the OpenJDK 17 installed via Homebrew\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "\n",
    "print(f\"‚úì JAVA_HOME set to: {java_home}\")\n",
    "print(\"‚úì Ready to run PySpark!\")\n",
    "print(f\"‚úì Current Python: {os.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "gnpyd-yh2ErZ",
   "metadata": {
    "id": "gnpyd-yh2ErZ"
   },
   "outputs": [],
   "source": [
    "# Setup: install and import\n",
    "# !pip install pyspark\n",
    "from pyspark.sql import SparkSession   # SparkSession is the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "from pyspark.sql.functions import *    # * = all\n",
    "from pyspark.sql.types import *        \n",
    "import pandas as pd                    \n",
    "import os                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "717ae97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/benjamingao/.venvs/pyspark-latest/bin/python\n",
      "Python version: 3.14.0 (main, Oct  7 2025, 09:34:52) [Clang 17.0.0 (clang-1700.3.19.1)]\n",
      "‚úÖ PySpark is installed: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# üîç Test: Verify Python environment\n",
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if PySpark is available\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"‚úÖ PySpark is installed: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PySpark NOT found! Wrong kernel selected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2252c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSTIC REPORT\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Python Environment:\n",
      "   Executable: /Users/benjamingao/.venvs/pyspark-latest/bin/python\n",
      "   Version: 3.14.0\n",
      "\n",
      "2Ô∏è‚É£ JAVA_HOME Setting:\n",
      "   JAVA_HOME: /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "\n",
      "3Ô∏è‚É£ Java Executable Test:\n",
      "   ‚úÖ Working Java found: /opt/homebrew/opt/openjdk@17/bin/java\n",
      "   Version: openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.17+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.17+0, mixed mode, sharing)\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ PySpark Installation:\n",
      "   ‚úÖ PySpark version: 4.0.1\n",
      "   Location: /Users/benjamingao/.venvs/pyspark-latest/lib/python3.14/site-packages/pyspark/__init__.py\n",
      "\n",
      "5Ô∏è‚É£ Spark Startup Test:\n",
      "   Attempting to start Spark...\n",
      "   ‚úÖ Spark started successfully!\n",
      "   Version: 4.0.1\n",
      "   Master: local[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:10:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Spark stopped successfully\n",
      "\n",
      "============================================================\n",
      "END OF DIAGNOSTIC REPORT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç Deep Diagnostics: Check Java and Spark Environment\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Check Python\n",
    "print(f\"\\n1Ô∏è‚É£ Python Environment:\")\n",
    "print(f\"   Executable: {sys.executable}\")\n",
    "print(f\"   Version: {sys.version.split()[0]}\")\n",
    "\n",
    "# 2. Check JAVA_HOME\n",
    "print(f\"\\n2Ô∏è‚É£ JAVA_HOME Setting:\")\n",
    "java_home = os.environ.get('JAVA_HOME', 'NOT SET')\n",
    "print(f\"   JAVA_HOME: {java_home}\")\n",
    "\n",
    "# 3. Check Java executable\n",
    "print(f\"\\n3Ô∏è‚É£ Java Executable Test:\")\n",
    "java_paths = [\n",
    "    \"/opt/homebrew/opt/openjdk@17/bin/java\",\n",
    "    \"/usr/bin/java\",\n",
    "    \"java\"\n",
    "]\n",
    "\n",
    "java_works = False\n",
    "for java_path in java_paths:\n",
    "    try:\n",
    "        result = subprocess.run([java_path, '-version'], \n",
    "                              capture_output=True, \n",
    "                              text=True, \n",
    "                              timeout=5)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ‚úÖ Working Java found: {java_path}\")\n",
    "            print(f\"   Version: {result.stderr.split('\\\\n')[0]}\")\n",
    "            java_works = True\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {java_path}: {str(e)[:50]}\")\n",
    "\n",
    "if not java_works:\n",
    "    print(\"\\n   ‚ö†Ô∏è WARNING: No working Java found!\")\n",
    "    print(\"   Solution: Install Java 17 with:\")\n",
    "    print(\"   brew install openjdk@17\")\n",
    "\n",
    "# 4. Check PySpark\n",
    "print(f\"\\n4Ô∏è‚É£ PySpark Installation:\")\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"   ‚úÖ PySpark version: {pyspark.__version__}\")\n",
    "    print(f\"   Location: {pyspark.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå PySpark not found: {e}\")\n",
    "\n",
    "# 5. Check if Spark can start\n",
    "print(f\"\\n5Ô∏è‚É£ Spark Startup Test:\")\n",
    "print(\"   Attempting to start Spark...\")\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    # Try to create with minimal config\n",
    "    test_spark = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"DiagnosticTest\") \\\n",
    "        .config(\"spark.ui.enabled\", \"false\") \\\n",
    "        .config(\"spark.driver.host\", \"localhost\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"   ‚úÖ Spark started successfully!\")\n",
    "    print(f\"   Version: {test_spark.version}\")\n",
    "    print(f\"   Master: {test_spark.sparkContext.master}\")\n",
    "    \n",
    "    # Clean up\n",
    "    test_spark.stop()\n",
    "    print(\"   ‚úÖ Spark stopped successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Spark failed to start:\")\n",
    "    print(f\"   Error: {str(e)[:200]}\")\n",
    "    print(\"\\n   üîß Recommended Fix:\")\n",
    "    print(\"   1. Restart Jupyter Kernel\")\n",
    "    print(\"   2. Run: brew reinstall openjdk@17\")\n",
    "    print(\"   3. Re-run all cells from the beginning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"END OF DIAGNOSTIC REPORT\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866c3f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "z08dUj8F2Era",
   "metadata": {
    "id": "z08dUj8F2Era"
   },
   "source": [
    "## 1. Introduction to PySpark\n",
    "\n",
    "### 1.1 What is PySpark?\n",
    "\n",
    "PySpark is the Python API for Apache Spark, a distributed computing engine for big data. It supports:\n",
    "- Distributed data processing\n",
    "- Fault tolerance\n",
    "- In-memory computations\n",
    "- Integration with many data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Wppg_ToU2Era",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wppg_ToU2Era",
    "outputId": "1fbffdba-55c7-40ea-b024-d8068ff1f586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing session to stop (this is fine)\n",
      "‚úì JAVA_HOME: /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "‚úÖ Spark version: 4.0.1\n",
      "‚úÖ Spark is running on: local[1]\n"
     ]
    }
   ],
   "source": [
    "# üîß Troubleshooting: Stop any existing Spark sessions first\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úì Stopped old Spark session\")\n",
    "except:\n",
    "    print(\"No existing session to stop (this is fine)\")\n",
    "\n",
    "# Re-verify JAVA_HOME is set\n",
    "import os\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "print(f\"‚úì JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Create SparkSession with additional configs to prevent connection issues\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkFundamentals\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.ui.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"‚úÖ Spark is running on: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530758e3",
   "metadata": {},
   "source": [
    "### 1.2 Creating a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf11fe",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation\n",
    "\n",
    "**SparkSession** (case sensitive)\n",
    "\n",
    "#### Why need session?\n",
    "Without session, the system does not recognize who you are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bb2f1",
   "metadata": {},
   "source": [
    "#### üè¶ Analogy: Bank Account\n",
    "\n",
    "| Code Part | Bank Analogy | Explanation |\n",
    "|-----------|--------------|-------------|\n",
    "| `SparkSession` | Opening a bank account | Your identity in the Spark system |\n",
    "| `.builder` | Walking into the bank | Starting the process |\n",
    "| `.appName(\"PySparkFundamentals\")` | Naming your account | Give your app a unique name |\n",
    "| `.config(...)` | Setting up special features | Configure how Spark behaves |\n",
    "| `.getOrCreate()` | Get existing OR create new | Smart! Reuses if exists, creates if not |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb38ce",
   "metadata": {},
   "source": [
    "#### üìã Step-by-Step Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Start building a SparkSession\n",
    "SparkSession.builder\n",
    "\n",
    "# Step 2: Name your application\n",
    ".appName(\"PySparkFundamentals\")\n",
    "\n",
    "# Step 3: Add configuration (optional)\n",
    ".config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "# ‚Üë This makes DataFrames auto-display in Jupyter\n",
    "\n",
    "# Step 4: Create or get existing session\n",
    ".getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe84702",
   "metadata": {},
   "source": [
    "#### üéØ Key Points\n",
    "\n",
    "- ‚úÖ **Case Sensitive**: Must write `SparkSession` (not `sparksession`)\n",
    "- ‚úÖ **One Session**: `getOrCreate()` ensures only one session exists\n",
    "- ‚úÖ **Required**: Without it, you can't use any Spark functions\n",
    "- ‚úÖ **Like Login**: It's your \"login credentials\" for Spark\n",
    "\n",
    "**This is called \"Method Chaining\"**\n",
    "- Object.Method1().Method2().Method3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99193",
   "metadata": {},
   "source": [
    "#### üß™ Quick Practice\n",
    "\n",
    "> ‰ªªÂä°ÔºöÂàõÂª∫‰∏Ä‰∏™ SparkSessionÔºåË¶ÅÊ±ÇÔºö\n",
    "> - Â∫îÁî®ÂêçÁß∞Ôºö\"StudentDataAnalysis\"ÔºàÂ≠¶ÁîüÊï∞ÊçÆÂàÜÊûêÔºâ\n",
    "> - ‰∏çÈúÄË¶ÅÈÖçÁΩÆ\n",
    "> - Â≠òÂÇ®Âà∞ÂèòÈáèÔºöspark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34295bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark2 version: 4.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 13:22:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark2 = SparkSession.builder.appName(\"StudentDataAnalysis\").getOrCreate()\n",
    "print(f\"Spark2 version: {spark2.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140903f",
   "metadata": {},
   "source": [
    "### 1.3 Understanding `getOrCreate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0d426",
   "metadata": {},
   "source": [
    "#### üéì Why It's Called `getOrCreate()`\n",
    "\n",
    "| Scenario | Behavior |\n",
    "|----------|----------|\n",
    "| **First Call** | **Create** - Creates a new SparkSession |\n",
    "| **Subsequent Calls** | **Get** - Returns existing SparkSession (ignores new config) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189efa5",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Important Discovery\n",
    "\n",
    "Run the verification code above and you'll find:\n",
    "- Both variables have the app name `\"PySparkFundamentals\"` (the first one created)\n",
    "- `spark is spark2` returns `True` (they are the same object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8090d",
   "metadata": {},
   "source": [
    "#### üìù What If You Really Want Multiple Sessions?\n",
    "\n",
    "**Method 1: Stop the old one, then create a new one** (within the same program)\n",
    "\n",
    "```python\n",
    "# Stop the old session\n",
    "spark.stop()\n",
    "\n",
    "# Create a new session\n",
    "spark2 = SparkSession.builder.appName(\"NewApp\").getOrCreate()\n",
    "```\n",
    "\n",
    "**Method 2: Run different programs** (recommended)\n",
    "\n",
    "Different Python scripts can each have their own SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27646ae0",
   "metadata": {},
   "source": [
    "#### üéØ Spark's Design Philosophy\n",
    "\n",
    "**One Application = One SparkSession = Multiple DataFrames**\n",
    "\n",
    "```python\n",
    "# ‚úÖ Correct approach: One Session, multiple datasets\n",
    "spark = SparkSession.builder.appName(\"DataAnalysis\").getOrCreate()\n",
    "\n",
    "# Process multiple types of data simultaneously\n",
    "students_df = spark.createDataFrame(student_data)\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "products_df = spark.createDataFrame(product_data)\n",
    "\n",
    "# Can analyze in parallel\n",
    "students_df.show()\n",
    "sales_df.show()\n",
    "products_df.show()\n",
    "```\n",
    "\n",
    "**You don't need multiple Sessions, you need multiple DataFrames!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e5925",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "### 2.1 From a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ccf8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+\n",
      "|     Name|Age|    Occupation|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 34, \"Engineer\"),\n",
    "        (\"Bob\", 45, \"Data Scientist\"),\n",
    "        (\"Catherine\", 29, \"Developer\"),\n",
    "        (\"David\", 52, \"Manager\")]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd0b02",
   "metadata": {},
   "source": [
    "#### üìù Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b091b91",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation: Creating DataFrame from a List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdf91b",
   "metadata": {},
   "source": [
    "#### üè¶ Analogy: Opening a Savings Account with Customer Records\n",
    "\n",
    "Imagine you work at a bank and need to digitize customer records:\n",
    "\n",
    "| Code Part | Bank Analogy | What's Happening |\n",
    "|-----------|--------------|------------------|\n",
    "| `data = [(...), (...), ...]` | **Customer info cards** | Raw data: each tuple is like a customer card with details |\n",
    "| `columns = [\"Name\", \"Age\", ...]` | **Form field labels** | Column headers: defining what each piece of data means |\n",
    "| `spark.createDataFrame(data, columns)` | **Create digital database** | Convert paper records into a structured database table |\n",
    "| `df` | **The customer database** | Your organized, searchable database |\n",
    "| `df.show()` | **Print the database** | Display the records on screen |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f520f",
   "metadata": {},
   "source": [
    "#### üìã Step-by-Step Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Prepare raw data (like customer info cards)\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\"),      # Card 1\n",
    "    (\"Bob\", 45, \"Data Scientist\"),  # Card 2\n",
    "    (\"Catherine\", 29, \"Developer\"), # Card 3\n",
    "    (\"David\", 52, \"Manager\")        # Card 4\n",
    "]\n",
    "# ‚Üë This is a LIST of TUPLES (each tuple = one row/record)\n",
    "\n",
    "# Step 2: Define column names (like form field labels)\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "# ‚Üë This is a LIST of STRINGS (column headers)\n",
    "\n",
    "# Step 3: Create a DataFrame (digitize the records)\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# ‚Üë spark: Your bank account (SparkSession)\n",
    "#   .createDataFrame(): The \"digitization machine\"\n",
    "#   data: What to digitize\n",
    "#   columns: How to label each field\n",
    "\n",
    "# Step 4: Display the database\n",
    "df.show()\n",
    "# ‚Üë Show the organized table on screen\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "APSQLONS2Erb",
   "metadata": {
    "id": "APSQLONS2Erb"
   },
   "source": [
    "### 2.2 From a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2UtnXGB22Erb",
   "metadata": {
    "id": "2UtnXGB22Erb"
   },
   "source": [
    "#### üêº What is Pandas?\n",
    "\n",
    "**Pandas** is a popular Python library for data analysis (like Excel for Python)\n",
    "\n",
    "- **Full Name**: Python Data Analysis Library\n",
    "- **Use Case**: Working with small-to-medium datasets (fits in your computer's memory)\n",
    "- **Key Object**: `DataFrame` - a table with rows and columns (like an Excel spreadsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cabff0",
   "metadata": {},
   "source": [
    "#### üí° Why Convert Pandas ‚Üí PySpark?\n",
    "\n",
    "You might have data in Pandas but want to:\n",
    "- Scale up to larger datasets\n",
    "- Use Spark's distributed processing\n",
    "- Integrate with existing Spark pipelines\n",
    "\n",
    "**Good News**: PySpark can easily convert Pandas DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4lajfUZ2Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4lajfUZ2Erc",
    "outputId": "f85f363b-79ac-4c4e-fe27-c5bf5955f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+\n",
      "| Product|Price|Quantity|\n",
      "+--------+-----+--------+\n",
      "|  Laptop| 1200|       5|\n",
      "|   Mouse|   25|      20|\n",
      "|Keyboard|   80|      15|\n",
      "| Monitor|  300|       8|\n",
      "+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    \"Product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"],\n",
    "    \"Price\": [1200, 25, 80, 300],\n",
    "    \"Quantity\": [5, 20, 15, 8]\n",
    "})\n",
    "\n",
    "products_df = spark.createDataFrame(pandas_df)\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yeWVUkXV2Erc",
   "metadata": {
    "id": "yeWVUkXV2Erc"
   },
   "source": [
    "### 2.3 Reading from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b971f",
   "metadata": {},
   "source": [
    "#### üìÑ What is CSV?\n",
    "\n",
    "**CSV = Comma-Separated Values**\n",
    "\n",
    "- A simple text file format for storing tabular data\n",
    "- Each line = one row\n",
    "- Commas separate columns\n",
    "- **Most common** format for big data exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "up133dQ02Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up133dQ02Erc",
    "outputId": "d71a1157-6002-49dc-d6ce-92b3e79398a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|value|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|  100|\n",
      "|  2|    Bob|  200|\n",
      "|  3|Charlie|  150|\n",
      "|  4|  Diana|  300|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15637dad",
   "metadata": {},
   "source": [
    "#### üìù Code Example: Creating and Reading CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aac69c",
   "metadata": {},
   "source": [
    "#### üîç Detailed Code Walkthrough\n",
    "\n",
    "Let me break down this code line by line:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b105c42",
   "metadata": {},
   "source": [
    "##### **Step 1: Create CSV Data (Line 1-5)**\n",
    "\n",
    "```python\n",
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- `csv_data` = a **variable** storing text\n",
    "- `\"\"\"...\"\"\"` = **triple quotes** (allows multi-line strings)\n",
    "- Content = CSV format data (comma-separated values)\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "Line 1: id,name,value        ‚Üê Header row (column names)\n",
    "Line 2: 1,Alice,100          ‚Üê Data row 1\n",
    "Line 3: 2,Bob,200            ‚Üê Data row 2\n",
    "Line 4: 3,Charlie,150        ‚Üê Data row 3\n",
    "Line 5: 4,Diana,300          ‚Üê Data row 4\n",
    "```\n",
    "\n",
    "**Analogy:** Like writing customer records on a piece of paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1264c3a",
   "metadata": {},
   "source": [
    "##### **Step 2: Create a Physical File (Line 7-8)**\n",
    "\n",
    "```python\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "| Part | Meaning | Analogy |\n",
    "|------|---------|---------|\n",
    "| `with open(...)` | Context manager (auto-closes file) | \"Use this file, then clean up automatically\" |\n",
    "| `\"sample_data.csv\"` | Filename to create | Name of the file on your computer |\n",
    "| `\"w\"` | Write mode | \"Create new file or overwrite existing one\" |\n",
    "| `as f:` | Give it a nickname `f` | Short name for the file object |\n",
    "| `f.write(csv_data)` | Write the text to file | Copy the text into the file |\n",
    "\n",
    "**What happens:**\n",
    "1. Creates (or overwrites) a file named `sample_data.csv`\n",
    "2. Writes the CSV text into it\n",
    "3. Automatically closes the file when done\n",
    "\n",
    "**Analogy:** Taking your paper records and putting them in a filing cabinet\n",
    "\n",
    "**Result:** You now have a real CSV file on your computer:\n",
    "```\n",
    "üìÅ Your Computer\n",
    "  ‚îî‚îÄ sample_data.csv  ‚Üê This file now exists!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454e8e6",
   "metadata": {},
   "source": [
    "##### **Step 3: Read CSV into PySpark (Line 10)**\n",
    "\n",
    "```python\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "| Part | What It Does |\n",
    "|------|--------------|\n",
    "| `csv_df =` | Store the result in variable `csv_df` |\n",
    "| `spark` | Your SparkSession (created earlier) |\n",
    "| `.read` | Access the DataFrameReader |\n",
    "| `.csv(...)` | Read a CSV file |\n",
    "| `\"sample_data.csv\"` | The file to read |\n",
    "| `header=True` | First row is column names |\n",
    "| `inferSchema=True` | Auto-detect data types |\n",
    "\n",
    "**Process:**\n",
    "```\n",
    "Step 1: spark.read\n",
    "        ‚Üì\n",
    "        Access Spark's file reader\n",
    "\n",
    "Step 2: .csv(\"sample_data.csv\")\n",
    "        ‚Üì\n",
    "        Read the CSV file\n",
    "\n",
    "Step 3: header=True\n",
    "        ‚Üì\n",
    "        Use first row as column names\n",
    "        (id, name, value)\n",
    "\n",
    "Step 4: inferSchema=True\n",
    "        ‚Üì\n",
    "        Figure out data types automatically\n",
    "        - id: integer\n",
    "        - name: string\n",
    "        - value: integer\n",
    "\n",
    "Step 5: Return DataFrame ‚Üí csv_df\n",
    "```\n",
    "\n",
    "**Analogy:** Scanning paper documents and creating a digital database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8f9f8",
   "metadata": {},
   "source": [
    "##### **Step 4: Display the DataFrame (Line 11)**\n",
    "\n",
    "```python\n",
    "csv_df.show()\n",
    "```\n",
    "\n",
    "**What it does:** Prints the DataFrame in a table format\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "+---+-------+-----+\n",
    "| id|   name|value|\n",
    "+---+-------+-----+\n",
    "|  1|  Alice|  100|\n",
    "|  2|    Bob|  200|\n",
    "|  3|Charlie|  150|\n",
    "|  4|  Diana|  300|\n",
    "+---+-------+-----+\n",
    "```\n",
    "\n",
    "**Analogy:** Printing a report to see your database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2764d0",
   "metadata": {},
   "source": [
    "##### üéØ Complete Flow Diagram\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 1: Create Text Data           ‚îÇ\n",
    "‚îÇ csv_data = \"\"\"id,name,value...\"\"\"  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 2: Write to File               ‚îÇ\n",
    "‚îÇ with open(\"sample_data.csv\", \"w\"):  ‚îÇ\n",
    "‚îÇ     f.write(csv_data)               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "        üìÅ sample_data.csv\n",
    "        (File on disk)\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 3: Read File into Spark        ‚îÇ\n",
    "‚îÇ csv_df = spark.read.csv(...)        ‚îÇ\n",
    "‚îÇ - header=True: Use first row        ‚îÇ\n",
    "‚îÇ - inferSchema=True: Detect types    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ csv_df (PySpark DataFrame)          ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îÇ | id|   name|value|                 ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îÇ |  1|  Alice|  100|                 ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 4: Display                     ‚îÇ\n",
    "‚îÇ csv_df.show()                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f4eeb",
   "metadata": {},
   "source": [
    "##### ü§î Why Two Steps (Create File + Read File)?\n",
    "\n",
    "**Question:** Why not read data directly from `csv_data` string?\n",
    "\n",
    "**Answer:** This example demonstrates the **real-world workflow**:\n",
    "1. In reality, CSV files already exist (from other systems)\n",
    "2. You just need to **read them** into Spark\n",
    "\n",
    "**Real-world scenario:**\n",
    "```python\n",
    "# You DON'T create the file, it already exists!\n",
    "# Just read it:\n",
    "df = spark.read.csv(\"sales_data_2024.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "This code creates a file just for **demonstration purposes** so you can practice reading CSVs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f994c",
   "metadata": {},
   "source": [
    "##### üí° Key Takeaways\n",
    "\n",
    "1. **CSV file** = text file with comma-separated values\n",
    "2. **`with open()`** = Python's way to create/write files\n",
    "3. **`spark.read.csv()`** = Spark's way to read CSV into DataFrame\n",
    "4. **`header=True`** = Treat first row as column names\n",
    "5. **`inferSchema=True`** = Auto-detect data types\n",
    "6. **`.show()`** = Display the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aef5bb",
   "metadata": {},
   "source": [
    "##### üìä Common Big Data CSV Scenarios\n",
    "\n",
    "**Scenario 1: Web Server Logs**\n",
    "```csv\n",
    "timestamp,ip_address,url,status_code\n",
    "2024-10-24 10:30:00,192.168.1.1,/home,200\n",
    "2024-10-24 10:31:15,192.168.1.2,/login,404\n",
    "```\n",
    "\n",
    "**Scenario 2: E-commerce Transactions**\n",
    "```csv\n",
    "order_id,customer_id,product,amount,date\n",
    "12345,C001,Laptop,1200,2024-10-24\n",
    "12346,C002,Mouse,25,2024-10-24\n",
    "```\n",
    "\n",
    "**Scenario 3: IoT Sensor Data**\n",
    "```csv\n",
    "sensor_id,temperature,humidity,timestamp\n",
    "S001,22.5,65,2024-10-24 10:00:00\n",
    "S002,23.1,62,2024-10-24 10:00:01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b3ed7",
   "metadata": {},
   "source": [
    "##### üéì CSV Best Practices\n",
    "\n",
    "‚úÖ **DO**:\n",
    "- Always use `header=True` if your CSV has headers\n",
    "- Use `inferSchema=True` for automatic type detection\n",
    "- Clean data before loading (remove extra spaces)\n",
    "\n",
    "‚ùå **DON'T**:\n",
    "- Assume data is clean (always check for spaces, nulls)\n",
    "- Load huge CSVs without partitioning (Spark handles this automatically)\n",
    "- Forget to handle special characters in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e82f9",
   "metadata": {},
   "source": [
    "##### ‚ö° Advanced CSV Options\n",
    "\n",
    "```python\n",
    "# Handle spaces around values\n",
    "df = spark.read.csv(\"data.csv\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    ignoreLeadingWhiteSpace=True,   # Remove leading spaces\n",
    "    ignoreTrailingWhiteSpace=True   # Remove trailing spaces\n",
    ")\n",
    "\n",
    "# Different delimiter (tab-separated)\n",
    "df = spark.read.csv(\"data.tsv\", sep=\"\\t\", header=True)\n",
    "\n",
    "# Handle missing values\n",
    "df = spark.read.csv(\"data.csv\", header=True, nullValue=\"N/A\")\n",
    "\n",
    "# Custom quote character\n",
    "df = spark.read.csv(\"data.csv\", header=True, quote=\"'\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3057",
   "metadata": {},
   "source": [
    "##### üîÑ Real-World CSV Loading\n",
    "\n",
    "**In production, you'll read files from:**\n",
    "\n",
    "```python\n",
    "# Local file\n",
    "df = spark.read.csv(\"file:///path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# HDFS (Hadoop Distributed File System)\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/data/sales.csv\", header=True)\n",
    "\n",
    "# AWS S3\n",
    "df = spark.read.csv(\"s3://my-bucket/data/logs.csv\", header=True)\n",
    "\n",
    "# Azure Blob Storage\n",
    "df = spark.read.csv(\"wasbs://container@account.blob.core.windows.net/data.csv\", header=True)\n",
    "\n",
    "# Google Cloud Storage\n",
    "df = spark.read.csv(\"gs://my-bucket/data/users.csv\", header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d934cf",
   "metadata": {},
   "source": [
    "##### üéØ Key Options Explained\n",
    "\n",
    "**`header=True`**\n",
    "- Treats the first line as column names\n",
    "- Without this, first line would be treated as data\n",
    "- Example:\n",
    "  ```\n",
    "  WITH header=True:     WITHOUT header=True:\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  | id|   name|value|   |_c0|    _c1|  _c2|\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  |  1|  Alice|  100|   | id|   name|value|\n",
    "  |  2|    Bob|  200|   |  1|  Alice|  100|\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  ```\n",
    "\n",
    "**`inferSchema=True`**\n",
    "- Automatically detects data types (int, string, double, etc.)\n",
    "- Without this, everything is treated as string\n",
    "- Example:\n",
    "  ```\n",
    "  WITH inferSchema=True:     WITHOUT inferSchema=True:\n",
    "  id: integer               id: string\n",
    "  name: string              name: string\n",
    "  value: integer            value: string\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8c290",
   "metadata": {},
   "source": [
    "##### üìã Complete Code Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Create CSV data as a string\n",
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "# ‚Üë Triple quotes allow multi-line strings\n",
    "#   First line: column names (header)\n",
    "#   Other lines: data rows\n",
    "\n",
    "# Step 2: Write CSV data to a file (creates \"sample_data.csv\")\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "# ‚Üë \"w\" = write mode (creates or overwrites file)\n",
    "#   This creates a real CSV file on your computer\n",
    "\n",
    "# Step 3: Read CSV file into PySpark DataFrame\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "#         ‚Üë      ‚Üë                         ‚Üë            ‚Üë\n",
    "#         |      |                         |            Auto-detect data types\n",
    "#         |      |                         First row is header\n",
    "#         |      Read CSV file\n",
    "#         Spark session\n",
    "\n",
    "# Step 4: Display the DataFrame\n",
    "csv_df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73fb871",
   "metadata": {},
   "source": [
    "##### üè¶ Analogy: Importing Bank Records from a File Cabinet\n",
    "\n",
    "Imagine you have customer records stored in a text file (CSV) and want to digitize them:\n",
    "\n",
    "| Code Part | Bank Analogy | What's Happening |\n",
    "|-----------|--------------|------------------|\n",
    "| `csv_data = \"\"\"...\"\"\"` | **Text file content** | The raw CSV data as a multi-line string |\n",
    "| `with open(...) as f:` | **Create a physical file** | Write the CSV text to a real file on disk |\n",
    "| `spark.read.csv(...)` | **Import file into database** | Read CSV file into a PySpark DataFrame |\n",
    "| `header=True` | **First row is column names** | Tells Spark the first line contains headers |\n",
    "| `inferSchema=True` | **Auto-detect data types** | Spark figures out which columns are numbers, strings, etc. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9fdc0",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation Summary\n",
    "\n",
    "This section provides a complete breakdown of how to read CSV files in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JCK-bupi2Erc",
   "metadata": {
    "id": "JCK-bupi2Erc"
   },
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "### 3.1 Show, Schema, Columns, Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "S_rYvCeg2Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_rYvCeg2Erc",
    "outputId": "5ba0cbe0-d568-4277-b62a-6f3ad7f1fbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+\n",
      "|     Name|Age|    Occupation|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n",
      "============================================================ 11111^^^^\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      "\n",
      "============================================================ 22222^^^^\n",
      "['Name', 'Age', 'Occupation']\n",
      "============================================================ 33333^^^^\n",
      "+-------+-----+------------------+--------------+\n",
      "|summary| Name|               Age|    Occupation|\n",
      "+-------+-----+------------------+--------------+\n",
      "|  count|    4|                 4|             4|\n",
      "|   mean| NULL|              40.0|          NULL|\n",
      "| stddev| NULL|10.424330514074594|          NULL|\n",
      "|    min|Alice|                29|Data Scientist|\n",
      "|    max|David|                52|       Manager|\n",
      "+-------+-----+------------------+--------------+\n",
      "\n",
      "============================================================ 44444^^^^\n",
      "+-------+-----+------------------+--------------+\n",
      "|summary| Name|               Age|    Occupation|\n",
      "+-------+-----+------------------+--------------+\n",
      "|  count|    4|                 4|             4|\n",
      "|   mean| NULL|              40.0|          NULL|\n",
      "| stddev| NULL|10.424330514074594|          NULL|\n",
      "|    min|Alice|                29|Data Scientist|\n",
      "|    max|David|                52|       Manager|\n",
      "+-------+-----+------------------+--------------+\n",
      "\n",
      "============================================================ 44444^^^^\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "print(\"=\"*60,\"11111^^^^\")\n",
    "df.printSchema()\n",
    "print(\"=\"*60,\"22222^^^^\")\n",
    "print(df.columns)\n",
    "print(\"=\"*60,\"33333^^^^\")\n",
    "df.describe().show()\n",
    "print(\"=\"*60,\"44444^^^^\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsu3hBmN2Erd",
   "metadata": {
    "id": "fsu3hBmN2Erd"
   },
   "source": [
    "### 3.2 Selecting and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "AzT81WpG2Erd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzT81WpG2Erd",
    "outputId": "dd0780d9-6435-41e9-c39e-ab6b97097229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|     Name|    Occupation|\n",
      "+---------+--------------+\n",
      "|    Alice|      Engineer|\n",
      "|      Bob|Data Scientist|\n",
      "|Catherine|     Developer|\n",
      "|    David|       Manager|\n",
      "+---------+--------------+\n",
      "\n",
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|      Engineer|\n",
      "|  Bob| 45|Data Scientist|\n",
      "|David| 52|       Manager|\n",
      "+-----+---+--------------+\n",
      "\n",
      "+-----+---+----------+\n",
      "| Name|Age|Occupation|\n",
      "+-----+---+----------+\n",
      "|Alice| 34|  Engineer|\n",
      "+-----+---+----------+\n",
      "\n",
      "+-----+---+----------+\n",
      "| Name|Age|Occupation|\n",
      "+-----+---+----------+\n",
      "|Alice| 34|  Engineer|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\", \"Occupation\").show()\n",
    "df.filter(df.Age > 30).show()\n",
    "df.filter((df.Age > 30) & (df.Occupation == \"Engineer\")).show()\n",
    "df.filter(\"Age > 30 AND Occupation = 'Engineer'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wtyYwt2J2Erd",
   "metadata": {
    "id": "wtyYwt2J2Erd"
   },
   "source": [
    "### 3.3 Adding / Modifying columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mnMk6gc12Erd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnMk6gc12Erd",
    "outputId": "e234a3d3-dac7-4bcd-8986-4d13b25c9a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+-----+\n",
      "|     Name|Age|    Occupation|Bonus|\n",
      "+---------+---+--------------+-----+\n",
      "|    Alice| 34|      Engineer|  340|\n",
      "|      Bob| 45|Data Scientist|  450|\n",
      "|Catherine| 29|     Developer|  290|\n",
      "|    David| 52|       Manager|  520|\n",
      "+---------+---+--------------+-----+\n",
      "\n",
      "+---------+---+--------------+\n",
      "|     Name|Age|           Job|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n",
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Alice| 34|\n",
      "|      Bob| 45|\n",
      "|Catherine| 29|\n",
      "|    David| 52|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_bonus = df.withColumn(\"Bonus\", df.Age * 10)\n",
    "df_with_bonus.show()\n",
    "\n",
    "df_renamed = df.withColumnRenamed(\"Occupation\", \"Job\")\n",
    "df_renamed.show()\n",
    "\n",
    "df_renamed.drop(\"Job\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waxM1lrL2zeR",
   "metadata": {
    "id": "waxM1lrL2zeR"
   },
   "source": [
    "## 4. Class Activity\n",
    "\n",
    "### üìä Practice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gLeGK_4V2-mt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLeGK_4V2-mt",
    "outputId": "9f48fd9e-f18b-460c-9bf0-f8cae59346f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\", 70000),\n",
    "    (\"Bob\", 45, \"Data Scientist\", 120000),\n",
    "    (\"Catherine\", 29, \"Developer\", 90000),\n",
    "    (\"David\", 52, \"Manager\", 150000),\n",
    "    (\"Eva\", 41, \"Engineer\", 80000),\n",
    "    (\"Frank\", 36, \"Developer\", 95000),\n",
    "    (\"Grace\", 28, \"Intern\", 40000)\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mQOzjbMu3xw2",
   "metadata": {
    "id": "mQOzjbMu3xw2"
   },
   "source": [
    "### üìù Activity Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "+---------+---+--------------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "|summary| Name|               Age|    Occupation|           Salary|\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "|  count|    7|                 7|             7|                7|\n",
      "|   mean| NULL|37.857142857142854|          NULL|92142.85714285714|\n",
      "| stddev| NULL| 8.706866474772873|          NULL|35338.49917313302|\n",
      "|    min|Alice|                28|Data Scientist|            40000|\n",
      "|    max|Grace|                52|       Manager|           150000|\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "\n",
      "============================================================ 11111^^^^\n"
     ]
    }
   ],
   "source": [
    "# - Show the first 5 rows \n",
    "# - Print the schema and column names\n",
    "# - Describe the dataset (count, mean, min, max)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "df.columns\n",
    "df.describe().show()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8557e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|Salary|\n",
      "+-----+------+\n",
      "|Alice| 70000|\n",
      "|  Eva| 80000|\n",
      "+-----+------+\n",
      "\n",
      "+-----+---+--------------+------+\n",
      "| Name|Age|    Occupation|Salary|\n",
      "+-----+---+--------------+------+\n",
      "|  Bob| 45|Data Scientist|120000|\n",
      "|Frank| 36|     Developer| 95000|\n",
      "+-----+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "# - Show only employees older than 35\n",
    "# - Show names and salaries of Engineers only\n",
    "# - Filter employees with salary > 90000 and age < 50\n",
    "df.filter(df.Occupation == \"Engineer\"  ).select(\"Name\",  \"Salary\").show()\n",
    "df.filter((df.Salary > 90000) & (df.Age < 50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95e7ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+-------+\n",
      "|     Name|Age|    Occupation|Salary|  Bonus|\n",
      "+---------+---+--------------+------+-------+\n",
      "|    Alice| 34|      Engineer| 70000| 7000.0|\n",
      "|      Bob| 45|Data Scientist|120000|12000.0|\n",
      "|Catherine| 29|     Developer| 90000| 9000.0|\n",
      "|    David| 52|       Manager|150000|15000.0|\n",
      "|      Eva| 41|      Engineer| 80000| 8000.0|\n",
      "|    Frank| 36|     Developer| 95000| 9500.0|\n",
      "|    Grace| 28|        Intern| 40000| 4000.0|\n",
      "+---------+---+--------------+------+-------+\n",
      "\n",
      "+---------+---+--------------+------+\n",
      "|     Name|Age|           Job|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n",
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n",
      "+---------+--------------+------+\n",
      "|     Name|    Occupation|Salary|\n",
      "+---------+--------------+------+\n",
      "|    Alice|      Engineer| 70000|\n",
      "|      Bob|Data Scientist|120000|\n",
      "|Catherine|     Developer| 90000|\n",
      "|    David|       Manager|150000|\n",
      "|      Eva|      Engineer| 80000|\n",
      "|    Frank|     Developer| 95000|\n",
      "|    Grace|        Intern| 40000|\n",
      "+---------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "# - Add a new column Bonus equal to 10% of Salary\n",
    "# - Rename the column Occupation to Job\n",
    "# - Drop the Age column from a copy of the DataFrame\n",
    "df_with_bonus = df.withColumn(\"Bonus\", df.Salary * 0.1)\n",
    "df_with_bonus.show()\n",
    "df_renamed = df.withColumnRenamed(\"Occupation\", \"Job\")\n",
    "df_renamed.show()\n",
    "df.show()\n",
    "dfnew = df.drop(\"Age\")\n",
    "dfnew.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f6272183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|    Occupation|Avg_Salary|\n",
      "+--------------+----------+\n",
      "|        Intern|   40000.0|\n",
      "|     Developer|   92500.0|\n",
      "|Data Scientist|  120000.0|\n",
      "|      Engineer|   75000.0|\n",
      "|       Manager|  150000.0|\n",
      "+--------------+----------+\n",
      "\n",
      "+--------------+--------------+\n",
      "|    Occupation|Employee_Count|\n",
      "+--------------+--------------+\n",
      "|        Intern|             1|\n",
      "|     Developer|             2|\n",
      "|Data Scientist|             1|\n",
      "|      Engineer|             2|\n",
      "|       Manager|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "# - Compute average salary per job (groupBy + avg)\n",
    "# - Count the number of employees in each job\n",
    "\n",
    "df2 = df.groupBy(\"Occupation\").agg(avg(\"Salary\").alias(\"Avg_Salary\"))\n",
    "df2.show()\n",
    "df2 = df.groupBy(\"Occupation\").agg(count(\"*\").alias(\"Employee_Count\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f8fee",
   "metadata": {},
   "source": [
    "## üìö PySpark Operations Summary\n",
    "\n",
    "### üîç Basic Display Operations\n",
    "\n",
    "#### Show Data\n",
    "```python\n",
    "df.show()           # Display all rows (default 20)\n",
    "df.show(n)          # Display first n rows\n",
    "```\n",
    "\n",
    "#### Inspect Structure\n",
    "```python\n",
    "df.printSchema()    # Print schema (column names and types)\n",
    "df.columns          # Show column names as a list\n",
    "df.describe().show() # Display statistical summary (count, mean, min, max)\n",
    "```\n",
    "\n",
    "**Note:** `df.describe()` returns a DataFrame object (not human-readable), use `.show()` to display it properly.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Filtering and Selecting\n",
    "\n",
    "#### Filter Rows + Select Columns\n",
    "```python\n",
    "df.filter(condition).select(col1, col2).show()\n",
    "```\n",
    "\n",
    "**Filter:** Apply conditions to keep certain rows\n",
    "- Supports logical operators: `&` (AND), `|` (OR), `~` (NOT)\n",
    "- Example: `df.filter((df.Age > 30) & (df.Salary > 80000))`\n",
    "\n",
    "**Select:** Choose which columns to display\n",
    "- Example: `df.select(\"Name\", \"Salary\")`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Column Transformations\n",
    "\n",
    "#### Add or Modify Column\n",
    "```python\n",
    "df.withColumn(\"new_col\", expression)\n",
    "```\n",
    "- If column exists ‚Üí modifies it\n",
    "- If column doesn't exist ‚Üí creates it\n",
    "- Example: `df.withColumn(\"Bonus\", df.Salary * 0.1)`\n",
    "\n",
    "#### Rename Column\n",
    "```python\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "```\n",
    "- Example: `df.withColumnRenamed(\"Occupation\", \"Job\")`\n",
    "\n",
    "#### Drop Column\n",
    "```python\n",
    "df.drop(\"column_name\")\n",
    "```\n",
    "- Example: `df.drop(\"Age\")`\n",
    "\n",
    "**Important:** These operations return a new DataFrame and don't modify the original!\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Aggregation Operations\n",
    "\n",
    "#### GroupBy + Aggregate\n",
    "```python\n",
    "df.groupBy(\"column\").agg(aggregation_function)\n",
    "```\n",
    "\n",
    "**Common Aggregation Functions:**\n",
    "\n",
    "| Function | Purpose | Example |\n",
    "|----------|---------|---------|\n",
    "| `avg(\"col\")` | Average | `avg(\"Salary\")` ‚Üí Average salary |\n",
    "| `sum(\"col\")` | Sum | `sum(\"Salary\")` ‚Üí Total salary |\n",
    "| `count(\"col\")` | Count non-null values | `count(\"Salary\")` ‚Üí Number of non-null salaries |\n",
    "| `count(\"*\")` | Count all rows | `count(\"*\")` ‚Üí Total number of rows |\n",
    "| `max(\"col\")` | Maximum | `max(\"Salary\")` ‚Üí Highest salary |\n",
    "| `min(\"col\")` | Minimum | `min(\"Salary\")` ‚Üí Lowest salary |\n",
    "\n",
    "#### Alias - Rename Result Column\n",
    "```python\n",
    "avg(\"Salary\").alias(\"Avg_Salary\")\n",
    "```\n",
    "- Makes result column names more readable\n",
    "- Example: Instead of `avg(Salary)`, displays as `Avg_Salary`\n",
    "\n",
    "**Complete Example:**\n",
    "```python\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    count(\"*\").alias(\"Employee_Count\")\n",
    ").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Concepts\n",
    "\n",
    "1. **Immutability:** All DataFrame operations return a new DataFrame\n",
    "2. **Method Chaining:** Can chain operations like `.filter().select().show()`\n",
    "3. **Lazy Evaluation:** Transformations aren't executed until an action (like `.show()`) is called\n",
    "4. **`.show()` Returns None:** Don't assign it to a variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5768b6",
   "metadata": {},
   "source": [
    "## üéØ Aggregation Practice Exercises\n",
    "\n",
    "Use the employee dataset above to complete these tasks:\n",
    "\n",
    "### Exercise 1: Basic Aggregations\n",
    "1. Find the **maximum salary** across all employees\n",
    "2. Find the **minimum age** across all employees\n",
    "3. Calculate the **total salary** (sum) for all employees\n",
    "\n",
    "### Exercise 2: GroupBy Aggregations\n",
    "1. Find the **maximum salary** for each Occupation\n",
    "2. Find the **minimum age** for each Occupation\n",
    "3. Count how many employees are in each Occupation\n",
    "\n",
    "### Exercise 3: Multiple Aggregations\n",
    "1. For each Occupation, show:\n",
    "   - Average salary\n",
    "   - Maximum salary\n",
    "   - Minimum salary\n",
    "   - Employee count\n",
    "   \n",
    "   (All in one query using multiple `.agg()` functions)\n",
    "\n",
    "### Exercise 4: Advanced Filtering + Aggregation\n",
    "1. Find the average salary for employees **older than 30**\n",
    "2. Count how many Engineers have salary **greater than 75000**\n",
    "3. For employees **under 40**, calculate average salary by Occupation\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Hints:**\n",
    "- Use `df.agg()` for aggregations on the entire dataset\n",
    "- Use `df.groupBy(\"col\").agg()` for aggregations per group\n",
    "- Remember to use `.alias()` to rename result columns\n",
    "- You can filter first with `.filter()`, then aggregate\n",
    "- For multiple aggregations, pass them separated by commas in `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ff6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Salary)|\n",
      "+-----------+\n",
      "|     150000|\n",
      "+-----------+\n",
      "\n",
      "+--------+\n",
      "|min(Age)|\n",
      "+--------+\n",
      "|      28|\n",
      "+--------+\n",
      "\n",
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     645000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Basic Aggregations\n",
    "# 1. Find the maximum salary across all employees\n",
    "df.agg(max(\"Salary\")).show()\n",
    "# 2. Find the minimum age across all employees\n",
    "df.agg(min(\"Age\")).show()\n",
    "# 3. Calculate the total salary (sum) for all employees\n",
    "df.agg(sum(\"Salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "149361a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Occupation|max(Salary)|\n",
      "+--------------+-----------+\n",
      "|        Intern|      40000|\n",
      "|     Developer|      95000|\n",
      "|Data Scientist|     120000|\n",
      "|      Engineer|      80000|\n",
      "|       Manager|     150000|\n",
      "+--------------+-----------+\n",
      "\n",
      "+--------------+--------+\n",
      "|    Occupation|min(Age)|\n",
      "+--------------+--------+\n",
      "|        Intern|      28|\n",
      "|     Developer|      29|\n",
      "|Data Scientist|      45|\n",
      "|      Engineer|      34|\n",
      "|       Manager|      52|\n",
      "+--------------+--------+\n",
      "\n",
      "+--------------+--------+\n",
      "|    Occupation|count(1)|\n",
      "+--------------+--------+\n",
      "|        Intern|       1|\n",
      "|     Developer|       2|\n",
      "|Data Scientist|       1|\n",
      "|      Engineer|       2|\n",
      "|       Manager|       1|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: GroupBy Aggregations\n",
    "# 1. Find the maximum salary for each Occupation\n",
    "df.groupBy(\"Occupation\").agg(max(\"Salary\")).show()\n",
    "\n",
    "# 2. Find the minimum age for each Occupation\n",
    "df.groupBy(\"Occupation\").agg(min(\"Age\")).show()\n",
    "\n",
    "# 3. Count how many employees are in each Occupation\n",
    "df.groupBy(\"Occupation\").agg(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbca066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----+-----+\n",
      "|    Occupation|  maxS|minA|count|\n",
      "+--------------+------+----+-----+\n",
      "|        Intern| 40000|  28|    1|\n",
      "|     Developer| 95000|  29|    2|\n",
      "|Data Scientist|120000|  45|    1|\n",
      "|      Engineer| 80000|  34|    2|\n",
      "|       Manager|150000|  52|    1|\n",
      "+--------------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Multiple Aggregations\n",
    "# For each Occupation, show: Average salary, Maximum salary, Minimum salary, Employee count\n",
    "# Hint: df.groupBy(\"col\").agg(func1.alias(\"name1\"), func2.alias(\"name2\"), ...)\n",
    "# Exercise 3: Multiple Aggregations\n",
    "# For each Occupation, show: Average salary, Maximum salary, Minimum salary, Employee count\n",
    "# Hint: df.groupBy(\"col\").agg(func1.alias(\"name1\"), func2.alias(\"name2\"), ...)\n",
    "\n",
    "# Complete solution with all 4 aggregations\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    max(\"Salary\").alias(\"Max_Salary\"),\n",
    "    min(\"Salary\").alias(\"Min_Salary\"),\n",
    "    count(\"*\").alias(\"Employee_Count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(Salary)|\n",
      "+-----------+\n",
      "|   103000.0|\n",
      "+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n",
      "+----------+-----------+\n",
      "|Occupation|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|    Intern|    40000.0|\n",
      "| Developer|    92500.0|\n",
      "|  Engineer|    70000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Advanced Filtering + Aggregation\n",
    "\n",
    "# 1. Find the average salary for employees older than 30\n",
    "df.filter(df.Age > 30).agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary_Age_Over_30\")\n",
    ").show()\n",
    "\n",
    "# 2. Count how many Engineers have salary greater than 75000\n",
    "df.filter((df.Occupation == \"Engineer\") & (df.Salary > 75000)).agg(\n",
    "    count(\"*\").alias(\"Engineer_Count_Salary_Over_75k\")\n",
    ").show()\n",
    "\n",
    "# 3. For employees under 40, calculate average salary by Occupation\n",
    "df.filter(df.Age < 40).groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d668c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Exercise Solutions Explanation\n",
    "\n",
    "### Exercise 3: Multiple Aggregations\n",
    "\n",
    "**Result Interpretation:**\n",
    "- Each row shows comprehensive statistics for one occupation\n",
    "- `Avg_Salary`: Average salary in that occupation\n",
    "- `Max_Salary`: Highest earner in that occupation\n",
    "- `Min_Salary`: Lowest earner in that occupation\n",
    "- `Employee_Count`: Total number of employees in that occupation\n",
    "\n",
    "**Key Learning:** You can compute multiple aggregations in a single query by passing them as comma-separated arguments to `.agg()`\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4: Advanced Filtering + Aggregation\n",
    "\n",
    "#### Question 1: Average Salary (Age > 30)\n",
    "**Result:** ~103,000\n",
    "- Filters out Grace (28) and Catherine (29)\n",
    "- Averages: Alice (70k), Bob (120k), Frank (95k), Eva (80k), David (150k)\n",
    "\n",
    "#### Question 2: Engineers with Salary > 75k\n",
    "**Result:** 1 engineer\n",
    "- Alice (70k) ‚ùå - Below threshold\n",
    "- Eva (80k) ‚úÖ - Above threshold\n",
    "- **Important:** Must filter by BOTH Occupation AND Salary\n",
    "\n",
    "#### Question 3: Average Salary by Occupation (Age < 40)\n",
    "**Results:**\n",
    "- Intern: 40,000 (only Grace, 28)\n",
    "- Developer: 92,500 (Catherine 29, Frank 36)\n",
    "- Engineer: 70,000 (only Alice, 34)\n",
    "- Bob (45) and David (52) are excluded ‚úÖ\n",
    "\n",
    "**Key Learning:** Filter first, then aggregate - this is method chaining in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28196516",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed all the fundamental PySpark operations! \n",
    "\n",
    "### üìö What You've Learned:\n",
    "\n",
    "#### ‚úÖ **1. SparkSession Management**\n",
    "- Creating and configuring SparkSession\n",
    "- Understanding `getOrCreate()` behavior\n",
    "- Managing Spark lifecycle\n",
    "\n",
    "#### ‚úÖ **2. DataFrame Creation (3 Methods)**\n",
    "- From Python lists\n",
    "- From Pandas DataFrames\n",
    "- Reading from CSV files\n",
    "\n",
    "#### ‚úÖ **3. Basic Operations**\n",
    "- Display: `show()`, `printSchema()`, `columns`, `describe()`\n",
    "- Filtering: Using conditions and logical operators\n",
    "- Selecting: Choosing specific columns\n",
    "- Transformations: Adding, renaming, dropping columns\n",
    "\n",
    "#### ‚úÖ **4. Aggregations**\n",
    "- Basic aggregations: `avg()`, `sum()`, `count()`, `max()`, `min()`\n",
    "- GroupBy operations\n",
    "- Multiple aggregations in one query\n",
    "- Combining filters with aggregations\n",
    "\n",
    "#### ‚úÖ **5. Key Concepts**\n",
    "- DataFrame immutability\n",
    "- Method chaining\n",
    "- Lazy evaluation\n",
    "- Proper use of `.alias()` for column naming\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Next Steps:\n",
    "\n",
    "1. **Advanced Transformations**\n",
    "   - Window functions\n",
    "   - User-defined functions (UDFs)\n",
    "   - Join operations\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Partitioning strategies\n",
    "   - Caching and persistence\n",
    "   - Broadcast variables\n",
    "\n",
    "3. **Real-World Projects**\n",
    "   - Log analysis\n",
    "   - Data ETL pipelines\n",
    "   - Machine learning with MLlib\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Additional Resources:\n",
    "\n",
    "- **Official Documentation**: [PySpark API Docs](https://spark.apache.org/docs/latest/api/python/)\n",
    "- **Spark SQL Guide**: [SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- **Community**: [Stack Overflow PySpark Tag](https://stackoverflow.com/questions/tagged/pyspark)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Spark Learning! üöÄ**\n",
    "\n",
    "If you found this helpful, consider ‚≠ê starring the repository!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyspark-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
