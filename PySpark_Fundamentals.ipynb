{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157592d8",
   "metadata": {},
   "source": [
    "# PySpark Fundamentals - Jupyter Notebook\n",
    "\n",
    "This notebook explains PySpark fundamentals for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006784e",
   "metadata": {},
   "source": [
    "## 0. Getting Started\n",
    "\n",
    "### 0.1 Acknowledgments\n",
    "\n",
    "**Original Source:** Professor Dr. Ali Safari  \n",
    "**Modified by:** Benjamin Gao (Enhanced structure and explanations for better learning experience)  \n",
    "**Email:** g103200@gmail.com \n",
    "\n",
    "---\n",
    "\n",
    "**‚òï Support This Work:**\n",
    "\n",
    "If you find this notebook helpful, consider buying me a coffee! Your support helps create more free educational content. üôè\n",
    "\n",
    "**üí≥ Fiat Currency:**\n",
    "- **Alipay/ÊîØ‰ªòÂÆù:** `g103200@gmail.com`\n",
    "- **Wise:** `g103200@gmail.com`\n",
    "- **PayPal:** [paypal.me/gbenjamin3](https://paypal.me/gbenjamin3)\n",
    "\n",
    "**‚Çø Cryptocurrency:**\n",
    "\n",
    "<details>\n",
    "<summary>Click to view crypto addresses</summary>\n",
    "\n",
    "- **ETH (ERC20):** `0x05bd3070993c1ef72b1ca3a06999cbcc3f61ad8b`\n",
    "- **USDT (ERC20):** `0x0a4649d6cbabf9bcf0419ac829f22a273136af51`\n",
    "- **SOL (Solana):** `3bsEtgBPeNwMrHLzQBrxiQ7wX1nr3dSRrzVAHoa1nudQ`\n",
    "- **BTC (Bitcoin):** `bc1ql0pafavp4l0l7j9m6dhgqajces3a80zqdj2kp8nua3aw4hqsm6vsnucv2m`\n",
    "\n",
    "</details>\n",
    "\n",
    "*Every contribution, no matter how small, is greatly appreciated!* ‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "### 0.2 How to Use This Notebook\n",
    "\n",
    "#### üìñ Folding Feature\n",
    "- Click **‚ñº** to collapse sections\n",
    "- Click **‚ñ∂** to expand content\n",
    "\n",
    "#### üéØ Learning Path\n",
    "1. Getting Started ‚Üí Setup environment\n",
    "2. Introduction ‚Üí PySpark basics\n",
    "3. Creating DataFrames ‚Üí Three creation methods\n",
    "4. Basic Operations ‚Üí Common operations\n",
    "5. Class Activity ‚Üí Hands-on practice\n",
    "\n",
    "---\n",
    "\n",
    "### 0.3 System Information\n",
    "\n",
    "**Local Environment (This Notebook):**\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|---------|\n",
    "| System | M4Pro MacBook Pro macOS 26 |\n",
    "| Python | 3.14.0 |\n",
    "| PySpark | 4.0.1 |\n",
    "| Java | OpenJDK 17 |\n",
    "| Environment | `~/.venvs/pyspark-latest` |\n",
    "\n",
    "**üí° Alternative: Google Colab**\n",
    "\n",
    "If you find local environment setup too complicated, consider using **Google Colab**:\n",
    "- ‚úÖ **Free** - No cost to use\n",
    "- ‚úÖ **Easy** - No installation needed, runs in browser\n",
    "- ‚úÖ **Powerful** - Free GPU/TPU access\n",
    "- ‚úÖ **Pre-configured** - PySpark ready to use with minimal setup\n",
    "\n",
    "**To use Colab:** Visit [colab.research.google.com](https://colab.research.google.com)\n",
    "\n",
    "---\n",
    "\n",
    "### 0.4 Kernel Selection\n",
    "\n",
    "**Current Setup: Python 3.14 (works fine!)**\n",
    "\n",
    "**Steps:**\n",
    "1. Click \"Select Kernel\" (top-right corner)\n",
    "2. Currently using **\"Python 3.14 (pyspark-latest)\"** ‚úÖ\n",
    "3. Alternative: **\"Python 3.12 (pyspark-py312)\"** is also available\n",
    "4. Run the setup cells below to configure JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1821918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JAVA_HOME: /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "‚úÖ Python: /Users/benjamingao/.venvs/pyspark-latest/bin/python\n",
      "‚úÖ Version: 3.14.0\n",
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è Setup: JAVA_HOME + Import Libraries (Run this FIRST!)\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "# Set JAVA_HOME to the OpenJDK 17 installed via Homebrew\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "\n",
    "# Import PySpark and other libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *  # All Spark SQL functions\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"‚úÖ JAVA_HOME: {java_home}\")\n",
    "print(f\"‚úÖ Python: {sys.executable}\")\n",
    "print(f\"‚úÖ Version: {sys.version.split()[0]}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "\n",
    "# üîç Optional: Quick Diagnostic (set to True to run)\n",
    "RUN_DIAGNOSTICS = False\n",
    "\n",
    "if RUN_DIAGNOSTICS:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIAGNOSTIC REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Python Environment\n",
    "    print(f\"\\n1Ô∏è‚É£ Python Environment:\")\n",
    "    print(f\"   Executable: {sys.executable}\")\n",
    "    print(f\"   Version: {sys.version.split()[0]}\")\n",
    "    \n",
    "    # 2. JAVA_HOME\n",
    "    print(f\"\\n2Ô∏è‚É£ JAVA_HOME:\")\n",
    "    print(f\"   {os.environ.get('JAVA_HOME', 'NOT SET')}\")\n",
    "    \n",
    "    # 3. Java Executable Test\n",
    "    print(f\"\\n3Ô∏è‚É£ Java Test:\")\n",
    "    java_paths = [\n",
    "        \"/opt/homebrew/opt/openjdk@17/bin/java\",\n",
    "        \"/usr/bin/java\",\n",
    "        \"java\"\n",
    "    ]\n",
    "    \n",
    "    for java_path in java_paths:\n",
    "        try:\n",
    "            result = subprocess.run([java_path, '-version'], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"   ‚úÖ {java_path}\")\n",
    "                print(f\"   {result.stderr.split(chr(10))[0]}\")\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # 4. PySpark Installation\n",
    "    print(f\"\\n4Ô∏è‚É£ PySpark:\")\n",
    "    try:\n",
    "        import pyspark\n",
    "        print(f\"   ‚úÖ Version {pyspark.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå Not installed\")\n",
    "    \n",
    "    # 5. Spark Startup Test\n",
    "    print(f\"\\n5Ô∏è‚É£ Spark Startup Test:\")\n",
    "    try:\n",
    "        test_spark = SparkSession.builder \\\n",
    "            .master(\"local[1]\") \\\n",
    "            .appName(\"DiagnosticTest\") \\\n",
    "            .config(\"spark.ui.enabled\", \"false\") \\\n",
    "            .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "            .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "            .config(\"spark.driver.port\", \"0\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        print(f\"   ‚úÖ Spark {test_spark.version} started!\")\n",
    "        print(f\"   ‚úÖ Master: {test_spark.sparkContext.master}\")\n",
    "        test_spark.stop()\n",
    "        print(\"   ‚úÖ Stopped successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to start\")\n",
    "        print(f\"\\n   Error Details:\")\n",
    "        print(\"   \" + \"‚îÄ\" * 56)\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        print(\"   \" + \"‚îÄ\" * 56)\n",
    "        print(f\"\\n   üí° Fix: Ensure network config uses 127.0.0.1\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5942cec",
   "metadata": {},
   "source": [
    "### üìã Quick Start\n",
    "\n",
    "**Run the cell below FIRST** - it will:\n",
    "- ‚úÖ Configure JAVA_HOME for Spark\n",
    "- ‚úÖ Import all required libraries (PySpark, pandas, etc.)\n",
    "- ‚úÖ Verify your setup (optional diagnostic available)\n",
    "\n",
    "**If you encounter errors:** Change `RUN_DIAGNOSTICS = False` to `True` for detailed diagnostic report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c37cfe",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Troubleshooting: Connection Refused Error\n",
    "\n",
    "**Problem:** `ConnectionRefusedError` when starting Spark\n",
    "\n",
    "**Root Cause:** macOS resolves `localhost` to IPv6 (`::1`) instead of IPv4 (`127.0.0.1`)\n",
    "\n",
    "**Solution:** Use explicit IPv4 binding (already applied in this notebook):\n",
    "```python\n",
    ".config(\"spark.driver.host\", \"127.0.0.1\")\n",
    ".config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "```\n",
    "\n",
    "**üí° Quick Diagnostic:** Set `RUN_DIAGNOSTICS = True` in the setup cell above if you encounter issues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c50dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark version: 4.0.1\n",
      "‚úÖ Spark is running on: local[1]\n"
     ]
    }
   ],
   "source": [
    "# Create (or get) SparkSession for this notebook\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"PySparkFundamentals\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.ui.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Be quiet in notebooks\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"‚úÖ Spark is running on: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z08dUj8F2Era",
   "metadata": {
    "id": "z08dUj8F2Era"
   },
   "source": [
    "## 1. Introduction to PySpark\n",
    "\n",
    "### 1.1 What is PySpark?\n",
    "\n",
    "PySpark is the Python API for Apache Spark, a distributed computing engine for big data. It supports:\n",
    "- Distributed data processing\n",
    "- Fault tolerance\n",
    "- In-memory computations\n",
    "- Integration with many data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530758e3",
   "metadata": {},
   "source": [
    "### 1.2 Creating a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf11fe",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation\n",
    "\n",
    "**SparkSession** (case sensitive)\n",
    "\n",
    "#### Why need session?\n",
    "Without session, the system does not recognize who you are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bb2f1",
   "metadata": {},
   "source": [
    "#### üè¶ Analogy: Bank Account\n",
    "\n",
    "| Code Part | Bank Analogy | Explanation |\n",
    "|-----------|--------------|-------------|\n",
    "| `SparkSession` | Opening a bank account | Your identity in the Spark system |\n",
    "| `.builder` | Walking into the bank | Starting the process |\n",
    "| `.appName(\"PySparkFundamentals\")` | Naming your account | Give your app a unique name |\n",
    "| `.config(...)` | Setting up special features | Configure how Spark behaves |\n",
    "| `.getOrCreate()` | Get existing OR create new | Smart! Reuses if exists, creates if not |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb38ce",
   "metadata": {},
   "source": [
    "#### üìã Step-by-Step Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Start building a SparkSession\n",
    "SparkSession.builder\n",
    "\n",
    "# Step 2: Name your application\n",
    ".appName(\"PySparkFundamentals\")\n",
    "\n",
    "# Step 3: Add configuration (optional)\n",
    ".config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "# ‚Üë This makes DataFrames auto-display in Jupyter\n",
    "\n",
    "# Step 4: Create or get existing session\n",
    ".getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe84702",
   "metadata": {},
   "source": [
    "#### üéØ Key Points\n",
    "\n",
    "- ‚úÖ **Case Sensitive**: Must write `SparkSession` (not `sparksession`)\n",
    "- ‚úÖ **One Session**: `getOrCreate()` ensures only one session exists\n",
    "- ‚úÖ **Required**: Without it, you can't use any Spark functions\n",
    "- ‚úÖ **Like Login**: It's your \"login credentials\" for Spark\n",
    "\n",
    "**This is called \"Method Chaining\"**\n",
    "- Object.Method1().Method2().Method3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99193",
   "metadata": {},
   "source": [
    "#### üß™ Quick Practice\n",
    "\n",
    "> **Task:** Create a SparkSession with the following requirements:\n",
    "> - Application name: \"StudentDataAnalysis\"\n",
    "> - No additional configuration needed\n",
    "> - Store in variable: spark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34295bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark2 version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "spark2 = SparkSession.builder.appName(\"StudentDataAnalysis\").getOrCreate()\n",
    "print(f\"Spark2 version: {spark2.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140903f",
   "metadata": {},
   "source": [
    "### 1.3 Understanding `getOrCreate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0d426",
   "metadata": {},
   "source": [
    "#### üéì Why It's Called `getOrCreate()`\n",
    "\n",
    "| Scenario | Behavior |\n",
    "|----------|----------|\n",
    "| **First Call** | **Create** - Creates a new SparkSession |\n",
    "| **Subsequent Calls** | **Get** - Returns existing SparkSession (ignores new config) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189efa5",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Important Discovery\n",
    "\n",
    "Run the verification code above and you'll find:\n",
    "- Both variables have the app name `\"PySparkFundamentals\"` (the first one created)\n",
    "- `spark is spark2` returns `True` (they are the same object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8090d",
   "metadata": {},
   "source": [
    "#### üìù What If You Really Want Multiple Sessions?\n",
    "\n",
    "**Method 1: Stop the old one, then create a new one** (within the same program)\n",
    "\n",
    "```python\n",
    "# Stop the old session\n",
    "spark.stop()\n",
    "\n",
    "# Create a new session\n",
    "spark2 = SparkSession.builder.appName(\"NewApp\").getOrCreate()\n",
    "```\n",
    "\n",
    "**Method 2: Run different programs** (recommended)\n",
    "\n",
    "Different Python scripts can each have their own SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27646ae0",
   "metadata": {},
   "source": [
    "#### üéØ Spark's Design Philosophy\n",
    "\n",
    "**One Application = One SparkSession = Multiple DataFrames**\n",
    "\n",
    "```python\n",
    "# ‚úÖ Correct approach: One Session, multiple datasets\n",
    "spark = SparkSession.builder.appName(\"DataAnalysis\").getOrCreate()\n",
    "\n",
    "# Process multiple types of data simultaneously\n",
    "students_df = spark.createDataFrame(student_data)\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "products_df = spark.createDataFrame(product_data)\n",
    "\n",
    "# Can analyze in parallel\n",
    "students_df.show()\n",
    "sales_df.show()\n",
    "products_df.show()\n",
    "```\n",
    "\n",
    "**You don't need multiple Sessions, you need multiple DataFrames!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e5925",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "### 2.1 From a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccf8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+\n",
      "|     Name|Age|    Occupation|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure a live SparkSession (recover if stopped or missing)\n",
    "try:\n",
    "    _sc = spark.sparkContext  # may raise NameError if spark isn't defined\n",
    "    if getattr(_sc, \"_jsc\", None) is None:\n",
    "        raise RuntimeError(\"SparkContext not active\")\n",
    "except Exception:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "            .master(\"local[1]\")\n",
    "            .appName(\"PySparkFundamentals\")\n",
    "            .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "            .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "            .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2.1 From a List\n",
    "data = [(\"Alice\", 34, \"Engineer\"),\n",
    "        (\"Bob\", 45, \"Data Scientist\"),\n",
    "        (\"Catherine\", 29, \"Developer\"),\n",
    "        (\"David\", 52, \"Manager\")]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd0b02",
   "metadata": {},
   "source": [
    "#### üìù Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b091b91",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation: Creating DataFrame from a List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdf91b",
   "metadata": {},
   "source": [
    "#### üè¶ Analogy: Opening a Savings Account with Customer Records\n",
    "\n",
    "Imagine you work at a bank and need to digitize customer records:\n",
    "\n",
    "| Code Part | Bank Analogy | What's Happening |\n",
    "|-----------|--------------|------------------|\n",
    "| `data = [(...), (...), ...]` | **Customer info cards** | Raw data: each tuple is like a customer card with details |\n",
    "| `columns = [\"Name\", \"Age\", ...]` | **Form field labels** | Column headers: defining what each piece of data means |\n",
    "| `spark.createDataFrame(data, columns)` | **Create digital database** | Convert paper records into a structured database table |\n",
    "| `df` | **The customer database** | Your organized, searchable database |\n",
    "| `df.show()` | **Print the database** | Display the records on screen |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f520f",
   "metadata": {},
   "source": [
    "#### üìã Step-by-Step Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Prepare raw data (like customer info cards)\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\"),      # Card 1\n",
    "    (\"Bob\", 45, \"Data Scientist\"),  # Card 2\n",
    "    (\"Catherine\", 29, \"Developer\"), # Card 3\n",
    "    (\"David\", 52, \"Manager\")        # Card 4\n",
    "]\n",
    "# ‚Üë This is a LIST of TUPLES (each tuple = one row/record)\n",
    "\n",
    "# Step 2: Define column names (like form field labels)\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "# ‚Üë This is a LIST of STRINGS (column headers)\n",
    "\n",
    "# Step 3: Create a DataFrame (digitize the records)\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# ‚Üë spark: Your bank account (SparkSession)\n",
    "#   .createDataFrame(): The \"digitization machine\"\n",
    "#   data: What to digitize\n",
    "#   columns: How to label each field\n",
    "\n",
    "# Step 4: Display the database\n",
    "df.show()\n",
    "# ‚Üë Show the organized table on screen\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "APSQLONS2Erb",
   "metadata": {
    "id": "APSQLONS2Erb"
   },
   "source": [
    "### 2.2 From a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2UtnXGB22Erb",
   "metadata": {
    "id": "2UtnXGB22Erb"
   },
   "source": [
    "#### üêº What is Pandas?\n",
    "\n",
    "**Pandas** is a popular Python library for data analysis (like Excel for Python)\n",
    "\n",
    "- **Full Name**: Python Data Analysis Library\n",
    "- **Use Case**: Working with small-to-medium datasets (fits in your computer's memory)\n",
    "- **Key Object**: `DataFrame` - a table with rows and columns (like an Excel spreadsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cabff0",
   "metadata": {},
   "source": [
    "#### üí° Why Convert Pandas ‚Üí PySpark?\n",
    "\n",
    "You might have data in Pandas but want to:\n",
    "- Scale up to larger datasets\n",
    "- Use Spark's distributed processing\n",
    "- Integrate with existing Spark pipelines\n",
    "\n",
    "**Good News**: PySpark can easily convert Pandas DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4lajfUZ2Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4lajfUZ2Erc",
    "outputId": "f85f363b-79ac-4c4e-fe27-c5bf5955f076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+\n",
      "| Product|Price|Quantity|\n",
      "+--------+-----+--------+\n",
      "|  Laptop| 1200|       5|\n",
      "|   Mouse|   25|      20|\n",
      "|Keyboard|   80|      15|\n",
      "| Monitor|  300|       8|\n",
      "+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    \"Product\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"],\n",
    "    \"Price\": [1200, 25, 80, 300],\n",
    "    \"Quantity\": [5, 20, 15, 8]\n",
    "})\n",
    "\n",
    "products_df = spark.createDataFrame(pandas_df)\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yeWVUkXV2Erc",
   "metadata": {
    "id": "yeWVUkXV2Erc"
   },
   "source": [
    "### 2.3 Reading from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b971f",
   "metadata": {},
   "source": [
    "#### üìÑ What is CSV?\n",
    "\n",
    "**CSV = Comma-Separated Values**\n",
    "\n",
    "- A simple text file format for storing tabular data\n",
    "- Each line = one row\n",
    "- Commas separate columns\n",
    "- **Most common** format for big data exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "up133dQ02Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up133dQ02Erc",
    "outputId": "d71a1157-6002-49dc-d6ce-92b3e79398a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|value|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|  100|\n",
      "|  2|    Bob|  200|\n",
      "|  3|Charlie|  150|\n",
      "|  4|  Diana|  300|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15637dad",
   "metadata": {},
   "source": [
    "#### üìù Code Example: Creating and Reading CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aac69c",
   "metadata": {},
   "source": [
    "#### üîç Detailed Code Walkthrough\n",
    "\n",
    "Let me break down this code line by line:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b105c42",
   "metadata": {},
   "source": [
    "##### **Step 1: Create CSV Data (Line 1-5)**\n",
    "\n",
    "```python\n",
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- `csv_data` = a **variable** storing text\n",
    "- `\"\"\"...\"\"\"` = **triple quotes** (allows multi-line strings)\n",
    "- Content = CSV format data (comma-separated values)\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "Line 1: id,name,value        ‚Üê Header row (column names)\n",
    "Line 2: 1,Alice,100          ‚Üê Data row 1\n",
    "Line 3: 2,Bob,200            ‚Üê Data row 2\n",
    "Line 4: 3,Charlie,150        ‚Üê Data row 3\n",
    "Line 5: 4,Diana,300          ‚Üê Data row 4\n",
    "```\n",
    "\n",
    "**Analogy:** Like writing customer records on a piece of paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1264c3a",
   "metadata": {},
   "source": [
    "##### **Step 2: Create a Physical File (Line 7-8)**\n",
    "\n",
    "```python\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "| Part | Meaning | Analogy |\n",
    "|------|---------|---------|\n",
    "| `with open(...)` | Context manager (auto-closes file) | \"Use this file, then clean up automatically\" |\n",
    "| `\"sample_data.csv\"` | Filename to create | Name of the file on your computer |\n",
    "| `\"w\"` | Write mode | \"Create new file or overwrite existing one\" |\n",
    "| `as f:` | Give it a nickname `f` | Short name for the file object |\n",
    "| `f.write(csv_data)` | Write the text to file | Copy the text into the file |\n",
    "\n",
    "**What happens:**\n",
    "1. Creates (or overwrites) a file named `sample_data.csv`\n",
    "2. Writes the CSV text into it\n",
    "3. Automatically closes the file when done\n",
    "\n",
    "**Analogy:** Taking your paper records and putting them in a filing cabinet\n",
    "\n",
    "**Result:** You now have a real CSV file on your computer:\n",
    "```\n",
    "üìÅ Your Computer\n",
    "  ‚îî‚îÄ sample_data.csv  ‚Üê This file now exists!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454e8e6",
   "metadata": {},
   "source": [
    "##### **Step 3: Read CSV into PySpark (Line 10)**\n",
    "\n",
    "```python\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "| Part | What It Does |\n",
    "|------|--------------|\n",
    "| `csv_df =` | Store the result in variable `csv_df` |\n",
    "| `spark` | Your SparkSession (created earlier) |\n",
    "| `.read` | Access the DataFrameReader |\n",
    "| `.csv(...)` | Read a CSV file |\n",
    "| `\"sample_data.csv\"` | The file to read |\n",
    "| `header=True` | First row is column names |\n",
    "| `inferSchema=True` | Auto-detect data types |\n",
    "\n",
    "**Process:**\n",
    "```\n",
    "Step 1: spark.read\n",
    "        ‚Üì\n",
    "        Access Spark's file reader\n",
    "\n",
    "Step 2: .csv(\"sample_data.csv\")\n",
    "        ‚Üì\n",
    "        Read the CSV file\n",
    "\n",
    "Step 3: header=True\n",
    "        ‚Üì\n",
    "        Use first row as column names\n",
    "        (id, name, value)\n",
    "\n",
    "Step 4: inferSchema=True\n",
    "        ‚Üì\n",
    "        Figure out data types automatically\n",
    "        - id: integer\n",
    "        - name: string\n",
    "        - value: integer\n",
    "\n",
    "Step 5: Return DataFrame ‚Üí csv_df\n",
    "```\n",
    "\n",
    "**Analogy:** Scanning paper documents and creating a digital database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8f9f8",
   "metadata": {},
   "source": [
    "##### **Step 4: Display the DataFrame (Line 11)**\n",
    "\n",
    "```python\n",
    "csv_df.show()\n",
    "```\n",
    "\n",
    "**What it does:** Prints the DataFrame in a table format\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "+---+-------+-----+\n",
    "| id|   name|value|\n",
    "+---+-------+-----+\n",
    "|  1|  Alice|  100|\n",
    "|  2|    Bob|  200|\n",
    "|  3|Charlie|  150|\n",
    "|  4|  Diana|  300|\n",
    "+---+-------+-----+\n",
    "```\n",
    "\n",
    "**Analogy:** Printing a report to see your database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2764d0",
   "metadata": {},
   "source": [
    "##### üéØ Complete Flow Diagram\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 1: Create Text Data           ‚îÇ\n",
    "‚îÇ csv_data = \"\"\"id,name,value...\"\"\"  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 2: Write to File               ‚îÇ\n",
    "‚îÇ with open(\"sample_data.csv\", \"w\"):  ‚îÇ\n",
    "‚îÇ     f.write(csv_data)               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "        üìÅ sample_data.csv\n",
    "        (File on disk)\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 3: Read File into Spark        ‚îÇ\n",
    "‚îÇ csv_df = spark.read.csv(...)        ‚îÇ\n",
    "‚îÇ - header=True: Use first row        ‚îÇ\n",
    "‚îÇ - inferSchema=True: Detect types    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ csv_df (PySpark DataFrame)          ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îÇ | id|   name|value|                 ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îÇ |  1|  Alice|  100|                 ‚îÇ\n",
    "‚îÇ +---+-------+-----+                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚îÇ\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Step 4: Display                     ‚îÇ\n",
    "‚îÇ csv_df.show()                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f4eeb",
   "metadata": {},
   "source": [
    "##### ü§î Why Two Steps (Create File + Read File)?\n",
    "\n",
    "**Question:** Why not read data directly from `csv_data` string?\n",
    "\n",
    "**Answer:** This example demonstrates the **real-world workflow**:\n",
    "1. In reality, CSV files already exist (from other systems)\n",
    "2. You just need to **read them** into Spark\n",
    "\n",
    "**Real-world scenario:**\n",
    "```python\n",
    "# You DON'T create the file, it already exists!\n",
    "# Just read it:\n",
    "df = spark.read.csv(\"sales_data_2024.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "This code creates a file just for **demonstration purposes** so you can practice reading CSVs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f994c",
   "metadata": {},
   "source": [
    "##### üí° Key Takeaways\n",
    "\n",
    "1. **CSV file** = text file with comma-separated values\n",
    "2. **`with open()`** = Python's way to create/write files\n",
    "3. **`spark.read.csv()`** = Spark's way to read CSV into DataFrame\n",
    "4. **`header=True`** = Treat first row as column names\n",
    "5. **`inferSchema=True`** = Auto-detect data types\n",
    "6. **`.show()`** = Display the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aef5bb",
   "metadata": {},
   "source": [
    "##### üìä Common Big Data CSV Scenarios\n",
    "\n",
    "**Scenario 1: Web Server Logs**\n",
    "```csv\n",
    "timestamp,ip_address,url,status_code\n",
    "2024-10-24 10:30:00,192.168.1.1,/home,200\n",
    "2024-10-24 10:31:15,192.168.1.2,/login,404\n",
    "```\n",
    "\n",
    "**Scenario 2: E-commerce Transactions**\n",
    "```csv\n",
    "order_id,customer_id,product,amount,date\n",
    "12345,C001,Laptop,1200,2024-10-24\n",
    "12346,C002,Mouse,25,2024-10-24\n",
    "```\n",
    "\n",
    "**Scenario 3: IoT Sensor Data**\n",
    "```csv\n",
    "sensor_id,temperature,humidity,timestamp\n",
    "S001,22.5,65,2024-10-24 10:00:00\n",
    "S002,23.1,62,2024-10-24 10:00:01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b3ed7",
   "metadata": {},
   "source": [
    "##### üéì CSV Best Practices\n",
    "\n",
    "‚úÖ **DO**:\n",
    "- Always use `header=True` if your CSV has headers\n",
    "- Use `inferSchema=True` for automatic type detection\n",
    "- Clean data before loading (remove extra spaces)\n",
    "\n",
    "‚ùå **DON'T**:\n",
    "- Assume data is clean (always check for spaces, nulls)\n",
    "- Load huge CSVs without partitioning (Spark handles this automatically)\n",
    "- Forget to handle special characters in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e82f9",
   "metadata": {},
   "source": [
    "##### ‚ö° Advanced CSV Options\n",
    "\n",
    "```python\n",
    "# Handle spaces around values\n",
    "df = spark.read.csv(\"data.csv\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    ignoreLeadingWhiteSpace=True,   # Remove leading spaces\n",
    "    ignoreTrailingWhiteSpace=True   # Remove trailing spaces\n",
    ")\n",
    "\n",
    "# Different delimiter (tab-separated)\n",
    "df = spark.read.csv(\"data.tsv\", sep=\"\\t\", header=True)\n",
    "\n",
    "# Handle missing values\n",
    "df = spark.read.csv(\"data.csv\", header=True, nullValue=\"N/A\")\n",
    "\n",
    "# Custom quote character\n",
    "df = spark.read.csv(\"data.csv\", header=True, quote=\"'\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3057",
   "metadata": {},
   "source": [
    "##### üîÑ Real-World CSV Loading\n",
    "\n",
    "**In production, you'll read files from:**\n",
    "\n",
    "```python\n",
    "# Local file\n",
    "df = spark.read.csv(\"file:///path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# HDFS (Hadoop Distributed File System)\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/data/sales.csv\", header=True)\n",
    "\n",
    "# AWS S3\n",
    "df = spark.read.csv(\"s3://my-bucket/data/logs.csv\", header=True)\n",
    "\n",
    "# Azure Blob Storage\n",
    "df = spark.read.csv(\"wasbs://container@account.blob.core.windows.net/data.csv\", header=True)\n",
    "\n",
    "# Google Cloud Storage\n",
    "df = spark.read.csv(\"gs://my-bucket/data/users.csv\", header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d934cf",
   "metadata": {},
   "source": [
    "##### üéØ Key Options Explained\n",
    "\n",
    "**`header=True`**\n",
    "- Treats the first line as column names\n",
    "- Without this, first line would be treated as data\n",
    "- Example:\n",
    "  ```\n",
    "  WITH header=True:     WITHOUT header=True:\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  | id|   name|value|   |_c0|    _c1|  _c2|\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  |  1|  Alice|  100|   | id|   name|value|\n",
    "  |  2|    Bob|  200|   |  1|  Alice|  100|\n",
    "  +---+-------+-----+   +---+-------+-----+\n",
    "  ```\n",
    "\n",
    "**`inferSchema=True`**\n",
    "- Automatically detects data types (int, string, double, etc.)\n",
    "- Without this, everything is treated as string\n",
    "- Example:\n",
    "  ```\n",
    "  WITH inferSchema=True:     WITHOUT inferSchema=True:\n",
    "  id: integer               id: string\n",
    "  name: string              name: string\n",
    "  value: integer            value: string\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8c290",
   "metadata": {},
   "source": [
    "##### üìã Complete Code Breakdown\n",
    "\n",
    "```python\n",
    "# Step 1: Create CSV data as a string\n",
    "csv_data = \"\"\"id,name,value\n",
    "1,Alice,100\n",
    "2,Bob,200\n",
    "3,Charlie,150\n",
    "4,Diana,300\"\"\"\n",
    "# ‚Üë Triple quotes allow multi-line strings\n",
    "#   First line: column names (header)\n",
    "#   Other lines: data rows\n",
    "\n",
    "# Step 2: Write CSV data to a file (creates \"sample_data.csv\")\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "# ‚Üë \"w\" = write mode (creates or overwrites file)\n",
    "#   This creates a real CSV file on your computer\n",
    "\n",
    "# Step 3: Read CSV file into PySpark DataFrame\n",
    "csv_df = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "#         ‚Üë      ‚Üë                         ‚Üë            ‚Üë\n",
    "#         |      |                         |            Auto-detect data types\n",
    "#         |      |                         First row is header\n",
    "#         |      Read CSV file\n",
    "#         Spark session\n",
    "\n",
    "# Step 4: Display the DataFrame\n",
    "csv_df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73fb871",
   "metadata": {},
   "source": [
    "##### üè¶ Analogy: Importing Bank Records from a File Cabinet\n",
    "\n",
    "Imagine you have customer records stored in a text file (CSV) and want to digitize them:\n",
    "\n",
    "| Code Part | Bank Analogy | What's Happening |\n",
    "|-----------|--------------|------------------|\n",
    "| `csv_data = \"\"\"...\"\"\"` | **Text file content** | The raw CSV data as a multi-line string |\n",
    "| `with open(...) as f:` | **Create a physical file** | Write the CSV text to a real file on disk |\n",
    "| `spark.read.csv(...)` | **Import file into database** | Read CSV file into a PySpark DataFrame |\n",
    "| `header=True` | **First row is column names** | Tells Spark the first line contains headers |\n",
    "| `inferSchema=True` | **Auto-detect data types** | Spark figures out which columns are numbers, strings, etc. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9fdc0",
   "metadata": {},
   "source": [
    "#### üí° Code Explanation Summary\n",
    "\n",
    "This section provides a complete breakdown of how to read CSV files in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JCK-bupi2Erc",
   "metadata": {
    "id": "JCK-bupi2Erc"
   },
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "### 3.1 Show, Schema, Columns, Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S_rYvCeg2Erc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_rYvCeg2Erc",
    "outputId": "5ba0cbe0-d568-4277-b62a-6f3ad7f1fbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+\n",
      "|     Name|Age|    Occupation|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      "\n",
      "Columns: ['Name', 'Age', 'Occupation']\n",
      "+-------+-----+------------------+--------------+\n",
      "|summary| Name|               Age|    Occupation|\n",
      "+-------+-----+------------------+--------------+\n",
      "|  count|    4|                 4|             4|\n",
      "|   mean| NULL|              40.0|          NULL|\n",
      "| stddev| NULL|10.424330514074594|          NULL|\n",
      "|    min|Alice|                29|Data Scientist|\n",
      "|    max|David|                52|       Manager|\n",
      "+-------+-----+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic inspection utilities\n",
    "# Show a small, clean summary of the DataFrame\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(\"Columns:\", df.columns)\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsu3hBmN2Erd",
   "metadata": {
    "id": "fsu3hBmN2Erd"
   },
   "source": [
    "### 3.2 Selecting and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AzT81WpG2Erd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzT81WpG2Erd",
    "outputId": "dd0780d9-6435-41e9-c39e-ab6b97097229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|     Name|    Occupation|\n",
      "+---------+--------------+\n",
      "|    Alice|      Engineer|\n",
      "|      Bob|Data Scientist|\n",
      "|Catherine|     Developer|\n",
      "|    David|       Manager|\n",
      "+---------+--------------+\n",
      "\n",
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|      Engineer|\n",
      "|  Bob| 45|Data Scientist|\n",
      "|David| 52|       Manager|\n",
      "+-----+---+--------------+\n",
      "\n",
      "+-----+---+----------+\n",
      "| Name|Age|Occupation|\n",
      "+-----+---+----------+\n",
      "|Alice| 34|  Engineer|\n",
      "+-----+---+----------+\n",
      "\n",
      "+-----+---+----------+\n",
      "| Name|Age|Occupation|\n",
      "+-----+---+----------+\n",
      "|Alice| 34|  Engineer|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\", \"Occupation\").show()\n",
    "df.filter(df.Age > 30).show()\n",
    "df.filter((df.Age > 30) & (df.Occupation == \"Engineer\")).show()\n",
    "df.filter(\"Age > 30 AND Occupation = 'Engineer'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wtyYwt2J2Erd",
   "metadata": {
    "id": "wtyYwt2J2Erd"
   },
   "source": [
    "### 3.3 Adding / Modifying columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mnMk6gc12Erd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnMk6gc12Erd",
    "outputId": "e234a3d3-dac7-4bcd-8986-4d13b25c9a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+-----+\n",
      "|     Name|Age|    Occupation|Bonus|\n",
      "+---------+---+--------------+-----+\n",
      "|    Alice| 34|      Engineer|  340|\n",
      "|      Bob| 45|Data Scientist|  450|\n",
      "|Catherine| 29|     Developer|  290|\n",
      "|    David| 52|       Manager|  520|\n",
      "+---------+---+--------------+-----+\n",
      "\n",
      "+---------+---+--------------+\n",
      "|     Name|Age|           Job|\n",
      "+---------+---+--------------+\n",
      "|    Alice| 34|      Engineer|\n",
      "|      Bob| 45|Data Scientist|\n",
      "|Catherine| 29|     Developer|\n",
      "|    David| 52|       Manager|\n",
      "+---------+---+--------------+\n",
      "\n",
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Alice| 34|\n",
      "|      Bob| 45|\n",
      "|Catherine| 29|\n",
      "|    David| 52|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_bonus = df.withColumn(\"Bonus\", df.Age * 10)\n",
    "df_with_bonus.show()\n",
    "\n",
    "df_renamed = df.withColumnRenamed(\"Occupation\", \"Job\")\n",
    "df_renamed.show()\n",
    "\n",
    "df_renamed.drop(\"Job\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waxM1lrL2zeR",
   "metadata": {
    "id": "waxM1lrL2zeR"
   },
   "source": [
    "## 4. Class Activity\n",
    "\n",
    "### üìä Practice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gLeGK_4V2-mt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLeGK_4V2-mt",
    "outputId": "9f48fd9e-f18b-460c-9bf0-f8cae59346f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\", 70000),\n",
    "    (\"Bob\", 45, \"Data Scientist\", 120000),\n",
    "    (\"Catherine\", 29, \"Developer\", 90000),\n",
    "    (\"David\", 52, \"Manager\", 150000),\n",
    "    (\"Eva\", 41, \"Engineer\", 80000),\n",
    "    (\"Frank\", 36, \"Developer\", 95000),\n",
    "    (\"Grace\", 28, \"Intern\", 40000)\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mQOzjbMu3xw2",
   "metadata": {
    "id": "mQOzjbMu3xw2"
   },
   "source": [
    "### üìù Activity Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "+---------+---+--------------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "|summary| Name|               Age|    Occupation|           Salary|\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "|  count|    7|                 7|             7|                7|\n",
      "|   mean| NULL|37.857142857142854|          NULL|92142.85714285714|\n",
      "| stddev| NULL| 8.706866474772873|          NULL|35338.49917313302|\n",
      "|    min|Alice|                28|Data Scientist|            40000|\n",
      "|    max|Grace|                52|       Manager|           150000|\n",
      "+-------+-----+------------------+--------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>summary</th><th>Name</th><th>Age</th><th>Occupation</th><th>Salary</th></tr>\n",
       "<tr><td>count</td><td>7</td><td>7</td><td>7</td><td>7</td></tr>\n",
       "<tr><td>mean</td><td>NULL</td><td>37.857142857142854</td><td>NULL</td><td>92142.85714285714</td></tr>\n",
       "<tr><td>stddev</td><td>NULL</td><td>8.706866474772873</td><td>NULL</td><td>35338.49917313302</td></tr>\n",
       "<tr><td>min</td><td>Alice</td><td>28</td><td>Data Scientist</td><td>40000</td></tr>\n",
       "<tr><td>max</td><td>Grace</td><td>52</td><td>Manager</td><td>150000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----+------------------+--------------+-----------------+\n",
       "|summary| Name|               Age|    Occupation|           Salary|\n",
       "+-------+-----+------------------+--------------+-----------------+\n",
       "|  count|    7|                 7|             7|                7|\n",
       "|   mean| NULL|37.857142857142854|          NULL|92142.85714285714|\n",
       "| stddev| NULL| 8.706866474772873|          NULL|35338.49917313302|\n",
       "|    min|Alice|                28|Data Scientist|            40000|\n",
       "|    max|Grace|                52|       Manager|           150000|\n",
       "+-------+-----+------------------+--------------+-----------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Show the first 5 rows \n",
    "# - Print the schema and column names\n",
    "# - Describe the dataset (count, mean, min, max)\n",
    "df.show(5)\n",
    "df.printSchema()\n",
    "df.columns\n",
    "df.describe().show()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8557e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|Salary|\n",
      "+-----+------+\n",
      "|Alice| 70000|\n",
      "|  Eva| 80000|\n",
      "+-----+------+\n",
      "\n",
      "+-----+---+--------------+------+\n",
      "| Name|Age|    Occupation|Salary|\n",
      "+-----+---+--------------+------+\n",
      "|  Bob| 45|Data Scientist|120000|\n",
      "|Frank| 36|     Developer| 95000|\n",
      "+-----+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "# - Show only employees older than 35\n",
    "# - Show names and salaries of Engineers only\n",
    "# - Filter employees with salary > 90000 and age < 50\n",
    "df.filter(df.Occupation == \"Engineer\"  ).select(\"Name\",  \"Salary\").show()\n",
    "df.filter((df.Salary > 90000) & (df.Age < 50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+------+-------+\n",
      "|     Name|Age|    Occupation|Salary|  Bonus|\n",
      "+---------+---+--------------+------+-------+\n",
      "|    Alice| 34|      Engineer| 70000| 7000.0|\n",
      "|      Bob| 45|Data Scientist|120000|12000.0|\n",
      "|Catherine| 29|     Developer| 90000| 9000.0|\n",
      "|    David| 52|       Manager|150000|15000.0|\n",
      "|      Eva| 41|      Engineer| 80000| 8000.0|\n",
      "|    Frank| 36|     Developer| 95000| 9500.0|\n",
      "|    Grace| 28|        Intern| 40000| 4000.0|\n",
      "+---------+---+--------------+------+-------+\n",
      "\n",
      "+---------+---+--------------+------+\n",
      "|     Name|Age|           Job|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n",
      "+---------+---+--------------+------+\n",
      "|     Name|Age|    Occupation|Salary|\n",
      "+---------+---+--------------+------+\n",
      "|    Alice| 34|      Engineer| 70000|\n",
      "|      Bob| 45|Data Scientist|120000|\n",
      "|Catherine| 29|     Developer| 90000|\n",
      "|    David| 52|       Manager|150000|\n",
      "|      Eva| 41|      Engineer| 80000|\n",
      "|    Frank| 36|     Developer| 95000|\n",
      "|    Grace| 28|        Intern| 40000|\n",
      "+---------+---+--------------+------+\n",
      "\n",
      "+---------+--------------+------+\n",
      "|     Name|    Occupation|Salary|\n",
      "+---------+--------------+------+\n",
      "|    Alice|      Engineer| 70000|\n",
      "|      Bob|Data Scientist|120000|\n",
      "|Catherine|     Developer| 90000|\n",
      "|    David|       Manager|150000|\n",
      "|      Eva|      Engineer| 80000|\n",
      "|    Frank|     Developer| 95000|\n",
      "|    Grace|        Intern| 40000|\n",
      "+---------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "# - Add a new column Bonus equal to 10% of Salary\n",
    "# - Rename the column Occupation to Job\n",
    "# - Drop the Age column from a copy of the DataFrame\n",
    "df_with_bonus = df.withColumn(\"Bonus\", df.Salary * 0.1)\n",
    "df_with_bonus.show()\n",
    "df_renamed = df.withColumnRenamed(\"Occupation\", \"Job\")\n",
    "df_renamed.show()\n",
    "df.show()\n",
    "dfnew = df.drop(\"Age\")\n",
    "dfnew.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6272183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|    Occupation|Avg_Salary|\n",
      "+--------------+----------+\n",
      "|        Intern|   40000.0|\n",
      "|     Developer|   92500.0|\n",
      "|Data Scientist|  120000.0|\n",
      "|      Engineer|   75000.0|\n",
      "|       Manager|  150000.0|\n",
      "+--------------+----------+\n",
      "\n",
      "+--------------+--------------+\n",
      "|    Occupation|Employee_Count|\n",
      "+--------------+--------------+\n",
      "|        Intern|             1|\n",
      "|     Developer|             2|\n",
      "|Data Scientist|             1|\n",
      "|      Engineer|             2|\n",
      "|       Manager|             1|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "# - Compute average salary per job (groupBy + avg)\n",
    "# - Count the number of employees in each job\n",
    "\n",
    "df2 = df.groupBy(\"Occupation\").agg(avg(\"Salary\").alias(\"Avg_Salary\"))\n",
    "df2.show()\n",
    "df2 = df.groupBy(\"Occupation\").agg(count(\"*\").alias(\"Employee_Count\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f8fee",
   "metadata": {},
   "source": [
    "## üìö PySpark Operations Summary\n",
    "\n",
    "### üîç Basic Display Operations\n",
    "\n",
    "#### Show Data\n",
    "```python\n",
    "df.show()           # Display all rows (default 20)\n",
    "df.show(n)          # Display first n rows\n",
    "```\n",
    "\n",
    "#### Inspect Structure\n",
    "```python\n",
    "df.printSchema()    # Print schema (column names and types)\n",
    "df.columns          # Show column names as a list\n",
    "df.describe().show() # Display statistical summary (count, mean, min, max)\n",
    "```\n",
    "\n",
    "**Note:** `df.describe()` returns a DataFrame object (not human-readable), use `.show()` to display it properly.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Filtering and Selecting\n",
    "\n",
    "#### Filter Rows + Select Columns\n",
    "```python\n",
    "df.filter(condition).select(col1, col2).show()\n",
    "```\n",
    "\n",
    "**Filter:** Apply conditions to keep certain rows\n",
    "- Supports logical operators: `&` (AND), `|` (OR), `~` (NOT)\n",
    "- Example: `df.filter((df.Age > 30) & (df.Salary > 80000))`\n",
    "\n",
    "**Select:** Choose which columns to display\n",
    "- Example: `df.select(\"Name\", \"Salary\")`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Column Transformations\n",
    "\n",
    "#### Add or Modify Column\n",
    "```python\n",
    "df.withColumn(\"new_col\", expression)\n",
    "```\n",
    "- If column exists ‚Üí modifies it\n",
    "- If column doesn't exist ‚Üí creates it\n",
    "- Example: `df.withColumn(\"Bonus\", df.Salary * 0.1)`\n",
    "\n",
    "#### Rename Column\n",
    "```python\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\")\n",
    "```\n",
    "- Example: `df.withColumnRenamed(\"Occupation\", \"Job\")`\n",
    "\n",
    "#### Drop Column\n",
    "```python\n",
    "df.drop(\"column_name\")\n",
    "```\n",
    "- Example: `df.drop(\"Age\")`\n",
    "\n",
    "**Important:** These operations return a new DataFrame and don't modify the original!\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Aggregation Operations\n",
    "\n",
    "#### GroupBy + Aggregate\n",
    "```python\n",
    "df.groupBy(\"column\").agg(aggregation_function)\n",
    "```\n",
    "\n",
    "**Common Aggregation Functions:**\n",
    "\n",
    "| Function | Purpose | Example |\n",
    "|----------|---------|---------|\n",
    "| `avg(\"col\")` | Average | `avg(\"Salary\")` ‚Üí Average salary |\n",
    "| `sum(\"col\")` | Sum | `sum(\"Salary\")` ‚Üí Total salary |\n",
    "| `count(\"col\")` | Count non-null values | `count(\"Salary\")` ‚Üí Number of non-null salaries |\n",
    "| `count(\"*\")` | Count all rows | `count(\"*\")` ‚Üí Total number of rows |\n",
    "| `max(\"col\")` | Maximum | `max(\"Salary\")` ‚Üí Highest salary |\n",
    "| `min(\"col\")` | Minimum | `min(\"Salary\")` ‚Üí Lowest salary |\n",
    "\n",
    "#### Alias - Rename Result Column\n",
    "```python\n",
    "avg(\"Salary\").alias(\"Avg_Salary\")\n",
    "```\n",
    "- Makes result column names more readable\n",
    "- Example: Instead of `avg(Salary)`, displays as `Avg_Salary`\n",
    "\n",
    "**Complete Example:**\n",
    "```python\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    count(\"*\").alias(\"Employee_Count\")\n",
    ").show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Concepts\n",
    "\n",
    "1. **Immutability:** All DataFrame operations return a new DataFrame\n",
    "2. **Method Chaining:** Can chain operations like `.filter().select().show()`\n",
    "3. **Lazy Evaluation:** Transformations aren't executed until an action (like `.show()`) is called\n",
    "4. **`.show()` Returns None:** Don't assign it to a variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5768b6",
   "metadata": {},
   "source": [
    "## üéØ Aggregation Practice Exercises\n",
    "\n",
    "Use the employee dataset above to complete these tasks:\n",
    "\n",
    "### Exercise 1: Basic Aggregations\n",
    "1. Find the **maximum salary** across all employees\n",
    "2. Find the **minimum age** across all employees\n",
    "3. Calculate the **total salary** (sum) for all employees\n",
    "\n",
    "### Exercise 2: GroupBy Aggregations\n",
    "1. Find the **maximum salary** for each Occupation\n",
    "2. Find the **minimum age** for each Occupation\n",
    "3. Count how many employees are in each Occupation\n",
    "\n",
    "### Exercise 3: Multiple Aggregations\n",
    "1. For each Occupation, show:\n",
    "   - Average salary\n",
    "   - Maximum salary\n",
    "   - Minimum salary\n",
    "   - Employee count\n",
    "   \n",
    "   (All in one query using multiple `.agg()` functions)\n",
    "\n",
    "### Exercise 4: Advanced Filtering + Aggregation\n",
    "1. Find the average salary for employees **older than 30**\n",
    "2. Count how many Engineers have salary **greater than 75000**\n",
    "3. For employees **under 40**, calculate average salary by Occupation\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Hints:**\n",
    "- Use `df.agg()` for aggregations on the entire dataset\n",
    "- Use `df.groupBy(\"col\").agg()` for aggregations per group\n",
    "- Remember to use `.alias()` to rename result columns\n",
    "- You can filter first with `.filter()`, then aggregate\n",
    "- For multiple aggregations, pass them separated by commas in `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ff6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Salary)|\n",
      "+-----------+\n",
      "|     150000|\n",
      "+-----------+\n",
      "\n",
      "+--------+\n",
      "|min(Age)|\n",
      "+--------+\n",
      "|      28|\n",
      "+--------+\n",
      "\n",
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     645000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Basic Aggregations\n",
    "# 1. Find the maximum salary across all employees\n",
    "df.agg(max(\"Salary\")).show()\n",
    "# 2. Find the minimum age across all employees\n",
    "df.agg(min(\"Age\")).show()\n",
    "# 3. Calculate the total salary (sum) for all employees\n",
    "df.agg(sum(\"Salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149361a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Occupation|max(Salary)|\n",
      "+--------------+-----------+\n",
      "|        Intern|      40000|\n",
      "|     Developer|      95000|\n",
      "|Data Scientist|     120000|\n",
      "|      Engineer|      80000|\n",
      "|       Manager|     150000|\n",
      "+--------------+-----------+\n",
      "\n",
      "+--------------+--------+\n",
      "|    Occupation|min(Age)|\n",
      "+--------------+--------+\n",
      "|        Intern|      28|\n",
      "|     Developer|      29|\n",
      "|Data Scientist|      45|\n",
      "|      Engineer|      34|\n",
      "|       Manager|      52|\n",
      "+--------------+--------+\n",
      "\n",
      "+--------------+--------+\n",
      "|    Occupation|count(1)|\n",
      "+--------------+--------+\n",
      "|        Intern|       1|\n",
      "|     Developer|       2|\n",
      "|Data Scientist|       1|\n",
      "|      Engineer|       2|\n",
      "|       Manager|       1|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: GroupBy Aggregations\n",
    "# 1. Find the maximum salary for each Occupation\n",
    "df.groupBy(\"Occupation\").agg(max(\"Salary\")).show()\n",
    "\n",
    "# 2. Find the minimum age for each Occupation\n",
    "df.groupBy(\"Occupation\").agg(min(\"Age\")).show()\n",
    "\n",
    "# 3. Count how many employees are in each Occupation\n",
    "df.groupBy(\"Occupation\").agg(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbca066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+----------+--------------+\n",
      "|    Occupation|Avg_Salary|Max_Salary|Min_Salary|Employee_Count|\n",
      "+--------------+----------+----------+----------+--------------+\n",
      "|        Intern|   40000.0|     40000|     40000|             1|\n",
      "|     Developer|   92500.0|     95000|     90000|             2|\n",
      "|Data Scientist|  120000.0|    120000|    120000|             1|\n",
      "|      Engineer|   75000.0|     80000|     70000|             2|\n",
      "|       Manager|  150000.0|    150000|    150000|             1|\n",
      "+--------------+----------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Multiple Aggregations\n",
    "# For each Occupation, show: Average salary, Maximum salary, Minimum salary, Employee count\n",
    "# Hint: df.groupBy(\"col\").agg(func1.alias(\"name1\"), func2.alias(\"name2\"), ...)\n",
    "\n",
    "# Complete solution with all 4 aggregations\n",
    "df.groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    max(\"Salary\").alias(\"Max_Salary\"),\n",
    "    min(\"Salary\").alias(\"Min_Salary\"),\n",
    "    count(\"*\").alias(\"Employee_Count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Avg_Salary_Age_Over_30|\n",
      "+----------------------+\n",
      "|              103000.0|\n",
      "+----------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|Engineer_Count_Salary_Over_75k|\n",
      "+------------------------------+\n",
      "|                             1|\n",
      "+------------------------------+\n",
      "\n",
      "+----------+----------+\n",
      "|Occupation|Avg_Salary|\n",
      "+----------+----------+\n",
      "|    Intern|   40000.0|\n",
      "| Developer|   92500.0|\n",
      "|  Engineer|   70000.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Advanced Filtering + Aggregation\n",
    "\n",
    "# 1. Find the average salary for employees older than 30\n",
    "df.filter(df.Age > 30).agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary_Age_Over_30\")\n",
    ").show()\n",
    "\n",
    "# 2. Count how many Engineers have salary greater than 75000\n",
    "df.filter((df.Occupation == \"Engineer\") & (df.Salary > 75000)).agg(\n",
    "    count(\"*\").alias(\"Engineer_Count_Salary_Over_75k\")\n",
    ").show()\n",
    "\n",
    "# 3. For employees under 40, calculate average salary by Occupation\n",
    "df.filter(df.Age < 40).groupBy(\"Occupation\").agg(\n",
    "    avg(\"Salary\").alias(\"Avg_Salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa111742",
   "metadata": {},
   "source": [
    "## üìö PySpark_Fundamentals_Part2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+\n",
      "|     Name| Age|Gender|\n",
      "+---------+----+------+\n",
      "|    Alice|  34|     M|\n",
      "|      Bob|  45|     M|\n",
      "|Catherine|  29|     F|\n",
      "|    David|  52|     M|\n",
      "|      Eva|NULL|     F|\n",
      "|    Frank|  36|  NULL|\n",
      "|    Grace|  28|     F|\n",
      "|    Henry|NULL|  NULL|\n",
      "|      Ivy|  33|     F|\n",
      "|     Jack|NULL|     M|\n",
      "|    Kathy|  27|     F|\n",
      "|      Leo|  44|     M|\n",
      "|     Mona|  31|  NULL|\n",
      "|     Nina|  38|     F|\n",
      "|     Nate|NULL|     M|\n",
      "|   Olivia|NULL|  NULL|\n",
      "+---------+----+------+\n",
      "\n",
      "+---------+----+\n",
      "|     Name| Age|\n",
      "+---------+----+\n",
      "|    Alice|  34|\n",
      "|      Bob|  45|\n",
      "|Catherine|  29|\n",
      "|    David|  52|\n",
      "|      Eva|NULL|\n",
      "|    Frank|  36|\n",
      "|    Grace|  28|\n",
      "|    Henry|NULL|\n",
      "|      Ivy|  33|\n",
      "|     Jack|NULL|\n",
      "|    Kathy|  27|\n",
      "|      Leo|  44|\n",
      "|     Mona|  31|\n",
      "|     Nina|  38|\n",
      "|     Nate|NULL|\n",
      "|   Olivia|NULL|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup & SparkSession\n",
    "spark3 = SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
    "data3 = [\n",
    "    (\"Alice\", 34, \"M\"),\n",
    "    (\"Bob\", 45, \"M\"),\n",
    "    (\"Catherine\", 29, \"F\"),\n",
    "    (\"David\", 52, \"M\"),\n",
    "    (\"Eva\", None, \"F\"),\n",
    "    (\"Frank\", 36, None),\n",
    "    (\"Grace\", 28, \"F\"),\n",
    "    (\"Henry\", None, None),\n",
    "    (\"Ivy\", 33, \"F\"),\n",
    "    (\"Jack\", None, \"M\"),\n",
    "    (\"Kathy\", 27, \"F\"),\n",
    "    (\"Leo\", 44, \"M\"),\n",
    "    (\"Mona\", 31, None),\n",
    "    (\"Nina\", 38, \"F\"),\n",
    "    (\"Nate\", None, \"M\"),\n",
    "    (\"Olivia\", None, None)\n",
    "]\n",
    "columns3 = [\"Name\", \"Age\", \"Gender\"]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df3 = spark3.createDataFrame(data3, columns3)\n",
    "df3.show()\n",
    "\n",
    "# 3. Select Columns\n",
    "df3.select(\"Name\", \"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cb1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4. Filter Rows\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m t1 = df3.filter((\u001b[43mdf3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAge\u001b[49m > \u001b[32m25\u001b[39m) & (df3.Gender.isNotNull()))\n\u001b[32m      3\u001b[39m t1.show()\n\u001b[32m      5\u001b[39m t2 = df3.filter((df3.Age > \u001b[32m25\u001b[39m) & (df3.Gender == \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m ))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:974\u001b[39m, in \u001b[36mDataFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns:\n\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkAttributeError(\n\u001b[32m    972\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mATTRIBUTE_NOT_SUPPORTED\u001b[39m\u001b[33m\"\u001b[39m, messageParameters={\u001b[33m\"\u001b[39m\u001b[33mattr_name\u001b[39m\u001b[33m\"\u001b[39m: name}\n\u001b[32m    973\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m jc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# 4. Filter Rows\n",
    "t1 = df3.filter((df3.Age > 25) & (df3.Gender.isNotNull()))\n",
    "t1.show()\n",
    "\n",
    "t2 = df3.filter((df3.Age > 25) & (df3.Gender == \"F\" ))\n",
    "t2.show()\n",
    "\n",
    "t3 = df3.filter((df3.Age!= 28) | (df3.Gender == \"M\"))\n",
    "t3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69425ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+---------+\n",
      "|     Name| Age|Gender|Age_plus1|\n",
      "+---------+----+------+---------+\n",
      "|    Alice|  34|     M|       35|\n",
      "|      Bob|  45|     M|       46|\n",
      "|Catherine|  29|     F|       30|\n",
      "|    David|  52|     M|       53|\n",
      "|      Eva|NULL|     F|     NULL|\n",
      "|    Frank|  36|  NULL|       37|\n",
      "|    Grace|  28|     F|       29|\n",
      "|    Henry|NULL|  NULL|     NULL|\n",
      "|      Ivy|  33|     F|       34|\n",
      "|     Jack|NULL|     M|     NULL|\n",
      "|    Kathy|  27|     F|       28|\n",
      "|      Leo|  44|     M|       45|\n",
      "|     Mona|  31|  NULL|       32|\n",
      "|     Nina|  38|     F|       39|\n",
      "|     Nate|NULL|     M|     NULL|\n",
      "|   Olivia|NULL|  NULL|     NULL|\n",
      "+---------+----+------+---------+\n",
      "\n",
      "+---------+----+------+-----+\n",
      "|     Name| Age|Gender|Age+1|\n",
      "+---------+----+------+-----+\n",
      "|    Alice|  34|     M|   35|\n",
      "|      Bob|  45|     M|   46|\n",
      "|Catherine|  29|     F|   30|\n",
      "|    David|  52|     M|   53|\n",
      "|      Eva|NULL|     F| NULL|\n",
      "|    Frank|  36|  NULL|   37|\n",
      "|    Grace|  28|     F|   29|\n",
      "|    Henry|NULL|  NULL| NULL|\n",
      "|      Ivy|  33|     F|   34|\n",
      "|     Jack|NULL|     M| NULL|\n",
      "|    Kathy|  27|     F|   28|\n",
      "|      Leo|  44|     M|   45|\n",
      "|     Mona|  31|  NULL|   32|\n",
      "|     Nina|  38|     F|   39|\n",
      "|     Nate|NULL|     M| NULL|\n",
      "|   Olivia|NULL|  NULL| NULL|\n",
      "+---------+----+------+-----+\n",
      "\n",
      "+---------+----+------+\n",
      "|     Name| Age|Gender|\n",
      "+---------+----+------+\n",
      "|    Alice|  34|     M|\n",
      "|      Bob|  45|     M|\n",
      "|Catherine|  29|     F|\n",
      "|    David|  52|     M|\n",
      "|      Eva|NULL|     F|\n",
      "|    Frank|  36|  NULL|\n",
      "|    Grace|  28|     F|\n",
      "|    Henry|NULL|  NULL|\n",
      "|      Ivy|  33|     F|\n",
      "|     Jack|NULL|     M|\n",
      "|    Kathy|  27|     F|\n",
      "|      Leo|  44|     M|\n",
      "|     Mona|  31|  NULL|\n",
      "|     Nina|  38|     F|\n",
      "|     Nate|NULL|     M|\n",
      "|   Olivia|NULL|  NULL|\n",
      "+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4. Add, Rename, Drop Columns\n",
    "df4 = df3.withColumn(\"Age_plus1\", df3.Age + 1)\n",
    "df4.show()\n",
    "\n",
    "df5 = df4.withColumnRenamed(\"Age_plus1\", \"Age+1\")\n",
    "df5.show()\n",
    "\n",
    "df6 = df5.drop(\"Age+1\")\n",
    "df6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     M|\n",
      "|      Bob| 45|     M|\n",
      "|Catherine| 29|     F|\n",
      "|    David| 52|     M|\n",
      "|    Grace| 28|     F|\n",
      "|      Ivy| 33|     F|\n",
      "|    Kathy| 27|     F|\n",
      "|      Leo| 44|     M|\n",
      "|     Nina| 38|     F|\n",
      "+---------+---+------+\n",
      "\n",
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     M|\n",
      "|      Bob| 45|     M|\n",
      "|Catherine| 29|     F|\n",
      "|    David| 52|     M|\n",
      "|      Eva| 28|     F|\n",
      "|    Frank| 36|     M|\n",
      "|    Grace| 28|     F|\n",
      "|    Henry| 28|     M|\n",
      "|      Ivy| 33|     F|\n",
      "|     Jack| 28|     M|\n",
      "|    Kathy| 27|     F|\n",
      "|      Leo| 44|     M|\n",
      "|     Mona| 31|     M|\n",
      "|     Nina| 38|     F|\n",
      "|     Nate| 28|     M|\n",
      "|   Olivia| 28|     M|\n",
      "+---------+---+------+\n",
      "\n",
      "36.09090909090909\n",
      "+---------+---+------+\n",
      "|     Name|Age|Gender|\n",
      "+---------+---+------+\n",
      "|    Alice| 34|     M|\n",
      "|      Bob| 45|     M|\n",
      "|Catherine| 29|     F|\n",
      "|    David| 52|     M|\n",
      "|      Eva| 36|     F|\n",
      "|    Frank| 36|  NULL|\n",
      "|    Grace| 28|     F|\n",
      "|    Henry| 36|  NULL|\n",
      "|      Ivy| 33|     F|\n",
      "|     Jack| 36|     M|\n",
      "|    Kathy| 27|     F|\n",
      "|      Leo| 44|     M|\n",
      "|     Mona| 31|  NULL|\n",
      "|     Nina| 38|     F|\n",
      "|     Nate| 36|     M|\n",
      "|   Olivia| 36|  NULL|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Handle Missing Values\n",
    "# 6.1 Drop Rows with Missing Values\n",
    "df_drop = df3.dropna()\n",
    "df_drop.show()\n",
    "# 6.2 fill all missing values with constants\n",
    "df_fconstants = df3.fillna({\"Age\":28, \"Gender\":\"M\", \"Name\": \"BigDaddy\"})\n",
    "df_fconstants.show()\n",
    "# 6.3 Fill missing numeric values with mean\n",
    "mean_value = df3.select(mean(\"Age\")).first()[0]\n",
    "print(mean_value) \n",
    "df_fmean = df3.fillna({\"Age\": mean_value})\n",
    "df_fmean.show()\n",
    "# 6.4 Fill missing categorical values with a suitable value\n",
    "# Here we can use 'Unknown' for missing names and gender\n",
    "df_fcat = df3.fillna({\"Name\": \"Unknown\", \"Gender\": \"Unknown\"})\n",
    "df_fcat.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd62053",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:527\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:530\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    529\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    531\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 7. Sort\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdf3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1055\u001b[39m         retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     logging.exception(\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/pyspark-latest/lib/python3.14/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# 7. Sort\n",
    "\n",
    "df3.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyspark-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
